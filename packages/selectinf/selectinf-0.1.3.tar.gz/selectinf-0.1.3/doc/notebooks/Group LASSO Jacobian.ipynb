{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import eigvalsh\n",
    "import functools\n",
    "import regreg.api as rr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jacobian for Group LASSO\n",
    "\n",
    "I want to finally get a working sampler for group LASSO.\n",
    "Let $\\pi:\\mathbb{R}^p \\rightarrow \\mathbb{R}^p$ denote the proximal map \n",
    "of our penalty ${\\cal P}$.\n",
    "\n",
    "The map, $z \\mapsto (\\pi(z), z - \\pi(z))$ is a bijection from $\\mathbb{R}^p$ to\n",
    "$$\n",
    "\\left\\{(\\beta, u): u \\in \\partial {\\cal P}(\\beta) \\right\\}.\n",
    "$$\n",
    "\n",
    "Our selective change of variables can then be expressed as\n",
    "$$\n",
    "\\omega(z;{\\cal D}) = \\nabla \\ell(\\beta; {\\cal D}) + \\epsilon \\beta+ u\n",
    "= \\nabla \\ell(\\pi(z)) + \\epsilon \\pi(z) + z - \\pi(z)\n",
    "$$\n",
    "\n",
    "The Jacobian is therefore\n",
    "$$\n",
    "\\left(\\nabla^2 \\ell(\\pi(z)) + (\\epsilon - 1) \\cdot I\\right) D_z\\pi(z) + I \n",
    "$$\n",
    "\n",
    "We know that\n",
    "$D\\pi(z)$ is block diagonal with $g$ block\n",
    "$$\n",
    "D_z\\pi(z)[g,g] = D_{z_g}\\left( \\frac{z_g}{\\|z_g\\|_2}(\\|z_g\\|_2 - \\lambda_g) \\right) = \n",
    "\\begin{cases}\n",
    "0 & \\|z_g\\|_2 \\leq \\lambda_g \\\\\n",
    "I_g - \\frac{\\lambda_g}{\\|z_g\\|_2} \\left(I - \\frac{1}{\\|z_g\\|^2_2}z_g z_g^T \\right) & \\|z_g\\|_2 > \\lambda_g\n",
    "\\end{cases}\n",
    "$$\n",
    "For a given active group $g$, our plan is to condition on $z_h, h \\neq g$.\n",
    "This might be easier to express in polar coordinates. Let \n",
    "$$\n",
    "(u_g(z_g), r_g(z_g)) = \\left(z_g / \\|z_g\\|_2, \\|z_g\\|_2 - \\lambda_g\\right)\n",
    "$$\n",
    "be our group specific polar coordinates so that the\n",
    "$(g,g)$ block of $D_z(\\pi(z))$ is (when non-zero)\n",
    "$$\n",
    "I_g - \\frac{\\lambda_g}{r_g + \\lambda_g} \\left(I - u_g u_g^T \\right) = u_gu_g^T + \\frac{r_g}{\\lambda_g + r_g} \\left(I_g - u_gu_g^T \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "Therefore, keeping $z_h, h \\neq g$ in standard coordinates\n",
    "and polar coordinates for $z_g$ the matrix takes the form\n",
    "$$\n",
    "\\left( \\nabla^2 \\ell(\\pi(z)) + (\\epsilon - 1) \\cdot I \\right)  \n",
    "\\begin{pmatrix}\n",
    "u_gu_g^T + \\frac{r_g}{\\lambda_g + r_g} \\left(I_g - u_gu_g^T \\right) & 0  & 0\\\\\n",
    "0 & \\left(u_hu_h^T + \\frac{r_h}{r_h + \\lambda_h} \\left(I_h - u_h u_h^T  \\right)\\right)_{h \\neq g \\in E} & 0 \\\\ 0 & 0 & 0\\\\\n",
    "\\end{pmatrix} + I\n",
    "= A(z) \\begin{pmatrix}\n",
    "u_gu_g^T + \\frac{r_g}{\\lambda_g + r_g} \\left(I_g - u_gu_g^T \\right) & 0  & 0\\\\\n",
    "0 & \\left(u_hu_h^T + \\frac{r_h}{r_h + \\lambda_h} \\left(I_h - u_h u_h^T \\right)\\right)_{h \\neq g \\in E} & 0 \\\\ 0 & 0 & 0\\\\\n",
    "\\end{pmatrix} + I\n",
    "$$\n",
    "\n",
    "Finally, we will condition on $u_g$ as well (we could try conditioning \n",
    "on its projective direction too, I suppose).\n",
    "So, ultimately we will just need to evaluate the determinant of this matrix\n",
    "as a function of $r_g$ (and integrate over $r_g$).\n",
    "\n",
    "Due to the block structure, we see that the determinant\n",
    "is the determinant of the smaller matrix\n",
    "$$\n",
    "A(z)[E,E] \\begin{pmatrix}\n",
    "u_gu_g^T + \\frac{r_g}{r_g + \\lambda_g} \\left(I_g - u_g u_g^T \\right) & 0  \\\\\n",
    "0 & \\left( u_hu_h^T + \\frac{r_h}{r_h + \\lambda_h} \\left(I_h - u_hu_h^T \\right)\\right)_{h \\neq g} \n",
    "\\end{pmatrix} + I_E\n",
    "$$\n",
    "\n",
    "We evaluate the matrix $A(z)[E, E]$ at $r_g=r_g^*$, calling this $A_0$, \n",
    "we see want the eigenvalues of\n",
    "$$\n",
    "A_0 \\begin{pmatrix}\n",
    "u_gu_g^T + \\frac{r^*_g}{r^*_g + \\lambda_g} \\left(I_g - u_gu_g^T \\right) & 0  \\\\\n",
    "0 & \\left( u_hu_h^T + \\frac{r_h}{r_h + \\lambda_h} \\left(I_h - u_hu_h^T \\right)\\right)_{h \\neq g} \n",
    "\\end{pmatrix} + \n",
    "A_0 \\begin{pmatrix}\n",
    "\\left(\\frac{r_g}{\\lambda_g + r_g} - \\frac{r_g^*}{\\lambda_g + r_g^*} \\right) (I_g - u_gu_g^T) & 0  \\\\\n",
    "0 & 0\n",
    "\\end{pmatrix} +\n",
    "I_E\n",
    " = A_0 (D_0 + c P) + I\n",
    "$$\n",
    "where \n",
    "$$\n",
    "c(r_g, r_g^*) = \\left(\\frac{r_g}{\\lambda_g + r_g} - \\frac{r_g^*}{r_g^*+\\lambda_g} \\right), \\qquad P = \\begin{pmatrix} I_g - u_gu_g^T & 0 \\\\ 0 & 0 \\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "We have used the approximation that $\\nabla^2 \\ell(\\pi(z))$ does not change\n",
    "noticably change with $r_g$ -- this is certainly true for least squares\n",
    "problems. Above $A_0$ is the matrix function $A(z_0)[E,E]$ evaluated at \n",
    "$z_0=(r_g^*, u_g, (z_h)_{h \\neq g})$ and\n",
    "$$\n",
    "P(z_0) = I - u_g u_g^T\n",
    "$$\n",
    "padded out appropriately to zero so it is of size $p$.\n",
    "We also know that only $|g|-1$ of these eigenvalues are non-zero and that\n",
    "$P$ commutes with $D_0$ (and hence $D_0^{\\pm 1/2}$ and $D_0^{-1}$ when\n",
    "these are symmetric square roots -- $D_0$ is symmetric because it is\n",
    "the Hessian of the value of a proximal problem).\n",
    "\n",
    "We want\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{det}(A_0 D_0 + c A_0 P + I) &= \n",
    "\\text{det}(D_0^{1/2} A_0 D_0^{1/2} + c D_0^{1/2} A_0 P D_0^{-1/2} + I) \\\\\n",
    "&= \\text{det}(D_0^{1/2} A_0 D_0^{1/2} + c D_0^{1/2} A_0 D_0^{-1/2} P + I) \\\\\n",
    "&= \\text{det}(D_0^{1/2} A_0 D_0^{1/2} + c D_0^{1/2} A_0 D_0^{1/2} D_0^{-1} P + I) \\\\\n",
    " &= \\text{det}(D_0^{1/2} A_0 D_0^{1/2} + I)^{-1} \\cdot \\text{det}(I + c(D_0^{1/2} A_0 D_0^{1/2} + I)^{-1}D_0^{1/2} A_0 D_0^{1/2} D_0^{-1} P)\n",
    "\\end{aligned}\n",
    "$$\n",
    "We see then that it is sufficient to find the eigenvalues of\n",
    "$$\n",
    "(D_0^{1/2} A_0 D_0^{1/2} + I)^{-1}D_0^{1/2} A_0 D_0^{1/2} D_0^{-1} P\n",
    "$$\n",
    "which is the product of two symmetric matrices. Hence all its eigenvalues are real and there are $|g|-1$ non-zero ones as the matrix $D_0^{-1}P$ is of rank $|g|-1$.\n",
    "\n",
    "Given these eigenvalues $\\gamma_j$ the determinant is\n",
    "$$\n",
    "\\text{det}(D_0^{1/2} A_0 D_0^{1/2} + I)^{-1} \\cdot \\prod_{j=1}^{|g|-1} \\left(1 + \\frac{r_g}{\\lambda_g + r_g} \\gamma_j\\right)\n",
    "$$\n",
    "and the first term will cancel in the integral.\n",
    "\n",
    "The eigenvalues of the above matrix are the top $|g|-1$ eigenvalues in\n",
    "the generalized eigenvalue problem\n",
    "$$\n",
    "D_0^{-1}Pv = \\gamma ( D_0^{1/2}A_0D_0^{1/2} + I) ( D_0^{1/2}A_0D_0^{1/2})^{-1}v.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Lastly, we should calculate $D_0^{1/2}$ and $D_0^{-1}$. For which we need\n",
    "the inverse and symmetric square-root of\n",
    "$$\n",
    "uu^T + \\frac{r}{\\lambda + r} \\left(I - u u^T\\right)?\n",
    "$$\n",
    "\n",
    "Hence, we see\n",
    "$$\n",
    "D_0^{-1/2} =  \\begin{pmatrix}\n",
    "u_gu_g^T + \\left(\\frac{\\lambda_g + r^*_g}{r_g}\\right)^{1/2} \\left(I_g - u_gu_g^T \\right) & 0  \\\\\n",
    "0 & \\left( u_hu_h^T + \\left(\\frac{\\lambda_h + r_h}{r_h}\\right)^{1/2} \\left(I_h - u_hu_h^T \\right)\\right)_{h \\neq g} \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Also,\n",
    "$$\n",
    "\\left(uu^T + \\frac{r}{\\lambda + r}(I - uu^T) \\right)^{-1} = uu^T + \\frac{\\lambda + r}{r} (I - uu^T)\n",
    "$$\n",
    "so that\n",
    "$$\n",
    "D_0^{-1}P = \\begin{pmatrix} \\frac{\\lambda_g + r^*_g}{r^*_g} (I_g - u_g u_g^T) & 0 \\\\ 0 & 0 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Going back to our generalized eigenvalue problem, we note a few things.\n",
    "First, when $|g|=1$, all eigenvalues are 0. Second, we note that\n",
    "any eigenvectors in this problem must be in $\\text{row}(P) = \\text{row}(I_g-u_gu_g^T) \\subset \\text{row}(I_g)$. Let $WW^T=I_g-u_gu_g^T$. Writing $v=WW^Tv$ and setting $\\eta=W^Tv$, the \n",
    "equation for the generalized eigenvalue problem reads\n",
    "$$\n",
    "\\begin{aligned}\n",
    "D_0^{-1}PW\\eta &= \\gamma ( D_0^{1/2}A_0D_0^{1/2} + I)  (D_0^{1/2}A_0D_0^{1/2})^{-1}W\\eta \\\\\n",
    "&= \\gamma (I + (D_0^{1/2}A_0D_0^{1/2})^{-1})W \\eta\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Multiplying both sides by $W^T$ yields\n",
    "$$\n",
    "\\begin{aligned}\n",
    "W^TD_0^{-1}PW\\eta &= \\frac{\\lambda_g+ r^*_g}{r^*_g}  \\eta \\\\\n",
    "&= \\gamma W^T ( I  + (D_0^{1/2}A_0D_0^{1/2})^{-1})W\\eta \\\\\n",
    "&= \\gamma (I + W^T(D_0^{1/2}A_0D_0^{1/2})^{-1}W)\\eta \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "Or,\n",
    "$$\n",
    "\\gamma W^T(D_0^{1/2}A_0D_0^{1/2})^{-1}W\\eta = \\left( \\frac{\\lambda_g+ r^*_g}{r^*_g} - \\gamma \\right) \\eta\n",
    "$$\n",
    " $\\eta$ is a (regular) eigenvector of \n",
    "$$\n",
    "W^T(D_0^{1/2}A_0D_0^{1/2})^{-1}W\n",
    "$$\n",
    "with eigenvalue \n",
    "$$\n",
    "\\frac{\\lambda_g + r^*_g}{ \\gamma r^*_g} - 1.$$\n",
    "\n",
    "(I think?) the non-zero eigenvalues of $W^T(D_0^{1/2}A_0D_0^{1/2})^{-1}W$ agree with those of\n",
    "$$\n",
    "P(D_0^{1/2}A_0D_0^{1/2})^{-1}P = D_0^{-1/2}PA_0^{-1}PD_0^{-1/2} = \n",
    "(I_g - u_gu_g^T)\\left(\\nabla^2 \\ell(\\pi(z_0)) + (\\epsilon - 1) I \\right)^{-1}[g,g](I_g - u_gu_g^T)\n",
    "$$\n",
    "\n",
    "\n",
    "Let $\\tilde{\\gamma}$ denote the (regular) eigenvalues of $W^T(D_0^{1/2}A_0D_0^{1/2})^{-1}W$, then\n",
    "$$\n",
    "\\gamma_j = \\frac{\\lambda_g + r_g^*}{(\\tilde{\\gamma}_j + 1) r^*_g}\n",
    "$$\n",
    "so that\n",
    "$$\n",
    "c(r_g, r_g^*) \\gamma_j = \\frac{\\lambda_g}{r_g^* (\\tilde{\\gamma}_j + 1)} \\frac{r_g - r_g^*}{r_g+\\lambda_g}.$$\n",
    "\n",
    "The ultimate determinant should not depend on the value $r_g^*$ chosen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def jacobian(hessian, \n",
    "             soln, \n",
    "             group_lasso_penalty, \n",
    "             randomization_precision, \n",
    "             tol=1.e-6, ff=1):\n",
    "    '''\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    hessian : the [:,E] block of the Hessian so hessian[E] is A_0 above\n",
    "    \n",
    "    group_id : a group index of group_lasso_penalty\n",
    "    \n",
    "    group_norm : $\\lambda_g$ above\n",
    "    \n",
    "    group_direction : $u_g$ above\n",
    "    \n",
    "    base_point: $r_g$ above\n",
    "    \n",
    "    Compute generalized eigenvalues above and return\n",
    "    function to evaluate jacobian as a function of $r_g=\\|z_g\\|_2$\n",
    "    fixing everything in the optimization variables except $r_g$.\n",
    "    \n",
    "    Above, $A_0$ is the Hessian of loss evaluated at an appropriate point.\n",
    "    '''\n",
    "    H, pen = hessian, group_lasso_penalty       # shorthand \n",
    "    nz = soln != 0                              # nonzero\n",
    "    nnz = nz.sum()                              # num nonzero\n",
    "    Hr = np.zeros((nnz, nnz))                            # restricted hessian   \n",
    "    sqrt_block = np.zeros((nnz, nnz))\n",
    "    group_idx = pen.groups == group_id\n",
    "    nz_groups = []\n",
    "\n",
    "    for idx in np.unique(pen.groups):\n",
    "        group_idx = pen.groups == idx\n",
    "        group_soln = soln[pen.groups == idx]\n",
    "        is_nz = np.linalg.norm(group_soln) > tol * np.linalg.norm(soln)\n",
    "        if is_nz:\n",
    "            ng = group_idx.sum()\n",
    "            group_direction = u_g = group_soln / np.linalg.norm(group_soln)\n",
    "            group_norm = r_g = ff * np.linalg.norm(group_soln)   # really r_g^*\n",
    "            group_weight = lambda_g = pen.weights[idx]\n",
    "            \n",
    "            fraction = np.sqrt(r_g / (lambda_g + r_g))\n",
    "            # one of the blocks in D_0^{1/2}\n",
    "            group_block = np.identity(ng) * fraction + (1 - fraction) * np.multiply.outer(u_g, u_g)\n",
    "            group_P = np.identity(ng) - np.multiply.outer(u_g, u_g)\n",
    "            nz_groups.append((idx, # a group index g\n",
    "                              group_idx, # indices where group==idx\n",
    "                              group_block, \n",
    "                              group_P,\n",
    "                              r_g,          \n",
    "                              lambda_g,\n",
    "                              group_direction)\n",
    "                            )\n",
    "            \n",
    "    # setup the block hessian Hr=D_0^{1/2}A_0D_0^{1/2}\n",
    "    \n",
    "    Q_blocks = []\n",
    "    ctr_g = 0\n",
    "    for group_g in nz_groups:\n",
    "        which_idx_g, block_g = group_g[1], group_g[2]\n",
    "        idx_g = slice(ctr_g, ctr_g + which_idx_g.sum())\n",
    "        Q_blocks.append(get_hessian(which_idx_g), slice(None, None))\n",
    "        ctr_h = 0\n",
    "        for group_h in nz_groups:\n",
    "            which_idx_h, block_h = group_h[1], group_h[2]\n",
    "            idx_h = slice(ctr_h, ctr_h + which_idx_h.sum())\n",
    "            H_hg = get_hessian(which_idx_h, which_idx_g)\n",
    "            Hr[idx_g][:,idx_h] += block_h.dot(H_hg).dot(block_g).T\n",
    "            ctr_h += which_idx_h.sum()\n",
    "        ctr_g += which_idx_g.sum()\n",
    "        \n",
    "    Q = np.vstack(Q_blocks)\n",
    "    implied_precision = Q.dot(randomization_precision).dot(Q.T)\n",
    "    \n",
    "    # compute (I+Hr)^{-1}Hr\n",
    "    \n",
    "    final_matrix = np.linalg.inv(Hr)\n",
    "    \n",
    "    ctr_g = 0\n",
    "    factors = []\n",
    "    ref_dens_info = {}\n",
    "    implied_variances = {}\n",
    "    for group_g in nz_groups:\n",
    "        which_g, which_idx_g, _, P_g, r_g, lambda_g, u_g = group_g\n",
    "        if which_idx_g.sum() > 1:\n",
    "            idx_g = slice(ctr_g, ctr_g + which_idx_g.sum())\n",
    "            block_g = final_matrix[idx_g][:,idx_g]\n",
    "            block_g = P_g.dot(block_g).dot(P_g)\n",
    "            eigvals_g = np.linalg.eigvalsh(block_g)[1:]               # \\tilde{\\gamma}'s\n",
    "            factors_g = lambda_g / ((eigvals_g + 1) * r_g)           # factors in the determinant\n",
    "            k_g = which_idx_g.sum()\n",
    "            def logdet_g(factors_g, r_g, k_g, lambda_g, r):\n",
    "                return (np.log(1 + np.multiply.outer(factors_g, r - r_g) / \n",
    "                               np.add.outer(lambda_g * \n",
    "                                            np.ones_like(factors_g), r)).sum(0)\n",
    "                        + np.log(lambda_g + r) * (k_g - 1))\n",
    "            logdet_g = functools.partial(logdet_g, factors_g, r_g, k_g, lambda_g)\n",
    "            \n",
    "            implied_variance = 1 / (u_g * implied_precision[idx_g][:,idx_g].dot(u_g)).sum()\n",
    "            ctr_g += which_idx_g.sum()\n",
    "        else: \n",
    "            logdet_g = lambda r: np.zeros_like(r)\n",
    "        \n",
    "        ref_dens_info[which_g] = (logdet_g, implied_variance)\n",
    "\n",
    "    return log_determinants\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [ 4.39444915  5.54517744  6.43775165  7.7836406 ]\n",
      "3 [ 5.20085193  5.77714647  6.22399156  6.8975735 ]\n",
      "5 [ 0.  0.  0.  0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathantaylor/anaconda/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:28: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    }
   ],
   "source": [
    "groups = [1]*5 + [2]*10 + [3]*3 + [4]*6 + [5]*1\n",
    "group_weights = {1:0, 2:1, 3:3.5, 4:2, 5:0.2}\n",
    "pen = rr.group_lasso(groups, group_weights, lagrange=1)\n",
    "soln = np.zeros(pen.shape)\n",
    "soln[:5] = np.random.standard_normal(5)\n",
    "soln[15:18] = np.random.standard_normal(3)\n",
    "soln[-1] = 2.\n",
    "pen.groups\n",
    "\n",
    "p = pen.shape[0]\n",
    "n = 100\n",
    "X = np.random.standard_normal((n, p))\n",
    "H = X.T.dot(X)\n",
    "\n",
    "def get_hessian(idx_i, idx_j):\n",
    "    return H[idx_i][:, idx_j]\n",
    "V0 = jacobian(get_hessian, soln, pen)\n",
    "for i in V0.keys():\n",
    "    print(i, V0[i](np.array([3.,4., 5., 7.])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking with a fudge factor to choose a different $r_g^*$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 [-0.50200638 -0.50197093 -0.50194976 -0.50192567]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathantaylor/anaconda/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:28: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    }
   ],
   "source": [
    "soln = np.zeros(pen.shape)\n",
    "soln[15:18] = np.random.standard_normal(3)\n",
    "V0 = jacobian(get_hessian, soln, pen)\n",
    "\n",
    "V1 = jacobian(get_hessian, soln, pen, ff=1.5)\n",
    "for i in V0.keys():\n",
    "    print(i, V1[i](np.array([3.,4., 5., 7.])) - V0[i](np.array([3.,4., 5., 7.]))) # these are log-dets -- subtract\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General penalties\n",
    "\n",
    "What we used crucially here to get into a generalized eigenvalue problem\n",
    "with symmetric matrices (i.e. real eigenvalues) was that $D_0$ commutes with $P$. If we want to condition on\n",
    "some function of optimization variables for general penalties (that are support functions of $K$) this property is not guaranteed. We will typically condition on the subgradient $u$ which\n",
    "fixes $N_uK$ and its dimension $d(u)$. Suppose we want to condition on $d(u)-1$ linear functions of the normal vector $\\beta$ -- this will correspond\n",
    "to taking an affine ray through $\\beta_0$ the observed $\\beta$. We will need that\n",
    "the Hessian of the prox along the ray $\\beta_{obs} + t \\alpha$ all have the same eigenspace. One direction $\\alpha$ that satisfies this is $\\alpha=\\beta/\\|\\beta\\|_2$. In the case $K$ is a product like the group LASSO we can find other examples. \n",
    "\n",
    "We also used the fact that $D_0$ was invertible. Generally the Hessian of the prox is not invertible, but it is invertible on the space spanned by its non-zero eigenvectors. This is what we used here in reducing the large block to a smaller block. Such a reduction will work generally -- under the assumption that $D_0$ and $P$ share the same eigenvectors corresponding to eigenvalue 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selective density\n",
    "\n",
    "Finally, let's pick a target and see how to compute appropriate reference densities.\n",
    "\n",
    "For group $g$, let $J'_g(r_g)=  J'_g(r_g;u, r_g^*, (\\beta_h)_{h \\neq g \\in E}); $ denote the determinant above. The map to polar coordinates picks up an extra factor of $(\\lambda_g + r_g )^{|g|-1}$.\n",
    "\n",
    "Let $$\n",
    "\\beta(r_g) = \\beta(r_g;u_g, (\\beta_h)_{h \\neq g \\in E}) = \\begin{pmatrix} r_g u_g \\\\ (\\beta_h)_{h \\neq g \\in E} = \n",
    "\\alpha_g r_g + \\kappa_g\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "We consider Gaussian randomization $\\omega$ having precision $\\Theta$, and for active set $E$ write\n",
    "$$\n",
    "\\nabla \\ell(\\beta_E) =  \\nabla\\ell(\\bar{\\beta}_E) + Q(\\bar{\\beta}_E)(\\beta_E- \\bar{\\beta}_E) = \\nabla \\ell(\\bar{\\beta}_E) -Q(\\bar{\\beta}_E)\\bar{\\beta}_E + Q\\beta_E\n",
    "$$\n",
    "where $\\nabla \\ell(\\bar{\\beta}_E)[E]=0$ and $\\bar{\\beta}_E, \\beta_E$ above are presumed filled out to \n",
    "be $p$-dimensional. Alternatively, we can take $Q=Q[:,E]$ to be only the active columns of the Hessian\n",
    "and then $\\bar{\\beta}_E$ and $\\beta_E$ are $E$-dimensional. In the argument of $\\nabla \\ell$, however, they will have to be padded. In any case, we see we never have to form the full $p \\times p$ matrix $Q$.\n",
    "\n",
    "The quantity $ \\nabla \\ell(\\bar{\\beta}_E) - Q(\\bar{\\beta}_E)\\bar{\\beta}_E$ is asymptotically equivalent (OK, low dim) to $ \\nabla\\ell(\\beta^*_E) - Q(\\beta^*_E)\\beta^*_E$ and is exactly $-X^TY$ in the linear regression setting. It is this quantity\n",
    "we linearly decompose as\n",
    "$$\n",
    "\\nabla \\ell(\\bar{\\beta}_E) + Q\\bar{\\beta}_E = N + AT.\n",
    "$$\n",
    "\n",
    "Hence, our reference distribution under $N(\\mu, \\Sigma)$ for target $T_g$ is proportional to (starting to drop $g$'s and $|g|=k$)\n",
    "$$\n",
    "\\phi_{(\\mu,\\Sigma)}(T) J'(r) (\\lambda + r)^{k-1} \\exp \\left(-\\frac{1}{2}\\left(N+AT+Q(\\alpha r + \\kappa)+u\\right)^T \\Theta \\left(N+AT+Q(\\alpha r + \\kappa)+u\\right)\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditional on $(T, N, \\alpha, u, \\kappa)$  this is (as a function of $r$) proportional to \n",
    "$$\n",
    "J'(r) (\\lambda + r)^{k-1} \\exp\\left(-\\frac{r^2 \\alpha^TQ^T\\Theta Q\\alpha}{2} - (N+AT+Q\\kappa+u)^T\\Theta Q\\alpha r\\right)\n",
    "1_{(0,\\infty)}(r)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $k=1$, this is a Gaussian density with variance\n",
    "$$\n",
    "\\frac{1}{\\alpha^TQ^T\\Theta Q\\alpha}\n",
    "$$\n",
    "and mean\n",
    "$$\n",
    "-\\frac{(N+AT+Q\\kappa+u)^T\\Theta Q\\alpha}{\\alpha^TQ^T\\Theta Q\\alpha}.\n",
    "$$\n",
    "\n",
    "Hence, the normalization is just\n",
    "$$\n",
    "1 - \\Phi \\left(\\frac{(N+AT+Q\\kappa+u)^T\\Theta Q\\alpha}{(\\alpha^TQ^T\\Theta Q\\alpha)^{1/2}}\\right).\n",
    "$$\n",
    "\n",
    "Therefore, for $k=1$ the appropriate reference density for target $T$ is proportional to\n",
    "$$\n",
    "t \\mapsto \\phi_{(\\mu,\\Sigma})(t) \\cdot \\left(1 - \\Phi \\left(\\frac{(N+AT+Q\\kappa+u)^T\\Theta Q\\alpha}{(\\alpha^TQ^T\\Theta Q\\alpha)^{1/2}}\\right) \\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $k > 1$, it is this Gaussian density, modified by the term $J'(r)(\\lambda+r)^{k-1}$.\n",
    "A cheap way to sample this would be to sample from the truncated Gaussian at some $T_0$ ($N$ will be fixed because we condition on it) and use importance weights.\n",
    "The appropriate reference density is proportional to\n",
    "$$\n",
    "t \\mapsto \\phi_{(\\mu,\\Sigma})(t) \\cdot \\int_0^{\\infty} J'(r) (\\lambda+r)^{k-1}\n",
    "\\exp\\left(-\\frac{r^2 \\alpha^TQ^T\\Theta Q\\alpha}{2} - (N + At + Q\\kappa + u)^T\\Theta Q\\alpha r\\right) \\; dr.\n",
    "$$\n",
    "or, for some $T_0$\n",
    "$$\n",
    "t \\mapsto \\phi_{(\\mu,\\Sigma})(t) \\cdot \\int_0^{\\infty} J'(r) (\\lambda+r)^{k-1} \\exp \\left((t-T_0)^TA^T\\Theta Q\\alpha r \\right)\n",
    "\\exp\\left(-\\frac{r^2 \\alpha^TQ^T\\Theta Q\\alpha}{2} - (N + AT_0 + Q\\kappa +u)^T\\Theta Q\\alpha r\\right) \\; dr.\n",
    "$$\n",
    "\n",
    "This second term can be evaluated as an expectation against a sample drawn from the above density at some reference $T_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data carving\n",
    "\n",
    "In the data carving scenario , we note that, at least in the correctly specified parametric selected model\n",
    "$$\n",
    "Q^T\\Theta Q = c(\\alpha) Q[E] \\in \\mathbb{R}^{E \\times E}\n",
    "$$\n",
    "i.e. it is a multiple of the precision matrix of the selected model, with the precision depending on the splitting proportion $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "randomization_variance = 1.\n",
    "prec = 1 / randomization_variance * np.identity(pen.shape[0])\n",
    "full_Q = get_hessian(soln != 0, slice(None, None))\n",
    "implied_precision = full_Q.dot(prec).dot(full_Q.T)\n",
    "\n",
    "implied_variances = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "all,-slideshow",
   "formats": "ipynb,Rmd"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
