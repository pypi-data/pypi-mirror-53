

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Selection &mdash; Selection Documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../_static/language_data.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/graphviz.css" type="text/css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />


</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html">
          

          
            
            <img src="../../_static/logo.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                0.1.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../docattribute.html">Selection documentation attribution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../algorithms/index.html">Non-randomized algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../randomized/index.html">Randomized algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../learning/index.html">Learning selection</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../documentation.html">Selection documentation</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">selection</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
      <li>Selection</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../_sources/api/generated/nipy.algorithms.statistics.models.regression.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast,
.nboutput.nblast {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast + .nbinput {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="algorithms-statistics-models-regression">
<h1>algorithms.statistics.models.regression<a class="headerlink" href="#algorithms-statistics-models-regression" title="Permalink to this headline">¶</a></h1>
<div class="section" id="module-algorithms-statistics-models-regression">
<h2>Module: <code class="xref py py-mod docutils literal notranslate"><span class="pre">algorithms.statistics.models.regression</span></code><a class="headerlink" href="#module-algorithms-statistics-models-regression" title="Permalink to this headline">¶</a></h2>
<p>Inheritance diagram for <code class="docutils literal notranslate"><span class="pre">nipy.algorithms.statistics.models.regression</span></code>:</p>
digraph inheritance7534ba070d {
rankdir=LR;
size=&quot;8.0, 12.0&quot;;
  &quot;models.model.LikelihoodModel&quot; [URL=&quot;nipy.algorithms.statistics.models.model.html#nipy.algorithms.statistics.models.model.LikelihoodModel&quot;,fontname=&quot;Vera Sans, DejaVu Sans, Liberation Sans, Arial, Helvetica, sans&quot;,fontsize=10,height=0.25,shape=box,style=&quot;setlinewidth(0.5)&quot;,target=&quot;_top&quot;];
  &quot;models.model.Model&quot; -&gt; &quot;models.model.LikelihoodModel&quot; [arrowsize=0.5,style=&quot;setlinewidth(0.5)&quot;];
  &quot;models.model.LikelihoodModelResults&quot; [URL=&quot;nipy.algorithms.statistics.models.model.html#nipy.algorithms.statistics.models.model.LikelihoodModelResults&quot;,fontname=&quot;Vera Sans, DejaVu Sans, Liberation Sans, Arial, Helvetica, sans&quot;,fontsize=10,height=0.25,shape=box,style=&quot;setlinewidth(0.5)&quot;,target=&quot;_top&quot;,tooltip=&quot;Class to contain results from likelihood models&quot;];
  &quot;models.model.Model&quot; [URL=&quot;nipy.algorithms.statistics.models.model.html#nipy.algorithms.statistics.models.model.Model&quot;,fontname=&quot;Vera Sans, DejaVu Sans, Liberation Sans, Arial, Helvetica, sans&quot;,fontsize=10,height=0.25,shape=box,style=&quot;setlinewidth(0.5)&quot;,target=&quot;_top&quot;,tooltip=&quot;A (predictive) statistical model.&quot;];
  &quot;models.regression.AREstimator&quot; [URL=&quot;#nipy.algorithms.statistics.models.regression.AREstimator&quot;,fontname=&quot;Vera Sans, DejaVu Sans, Liberation Sans, Arial, Helvetica, sans&quot;,fontsize=10,height=0.25,shape=box,style=&quot;setlinewidth(0.5)&quot;,target=&quot;_top&quot;,tooltip=&quot;A class to estimate AR(p) coefficients from residuals&quot;];
  &quot;models.regression.ARModel&quot; [URL=&quot;#nipy.algorithms.statistics.models.regression.ARModel&quot;,fontname=&quot;Vera Sans, DejaVu Sans, Liberation Sans, Arial, Helvetica, sans&quot;,fontsize=10,height=0.25,shape=box,style=&quot;setlinewidth(0.5)&quot;,target=&quot;_top&quot;,tooltip=&quot;A regression model with an AR(p) covariance structure.&quot;];
  &quot;models.regression.OLSModel&quot; -&gt; &quot;models.regression.ARModel&quot; [arrowsize=0.5,style=&quot;setlinewidth(0.5)&quot;];
  &quot;models.regression.GLSModel&quot; [URL=&quot;#nipy.algorithms.statistics.models.regression.GLSModel&quot;,fontname=&quot;Vera Sans, DejaVu Sans, Liberation Sans, Arial, Helvetica, sans&quot;,fontsize=10,height=0.25,shape=box,style=&quot;setlinewidth(0.5)&quot;,target=&quot;_top&quot;,tooltip=&quot;Generalized least squares model with a general covariance structure&quot;];
  &quot;models.regression.OLSModel&quot; -&gt; &quot;models.regression.GLSModel&quot; [arrowsize=0.5,style=&quot;setlinewidth(0.5)&quot;];
  &quot;models.regression.OLSModel&quot; [URL=&quot;#nipy.algorithms.statistics.models.regression.OLSModel&quot;,fontname=&quot;Vera Sans, DejaVu Sans, Liberation Sans, Arial, Helvetica, sans&quot;,fontsize=10,height=0.25,shape=box,style=&quot;setlinewidth(0.5)&quot;,target=&quot;_top&quot;,tooltip=&quot;A simple ordinary least squares model.&quot;];
  &quot;models.model.LikelihoodModel&quot; -&gt; &quot;models.regression.OLSModel&quot; [arrowsize=0.5,style=&quot;setlinewidth(0.5)&quot;];
  &quot;models.regression.RegressionResults&quot; [URL=&quot;#nipy.algorithms.statistics.models.regression.RegressionResults&quot;,fontname=&quot;Vera Sans, DejaVu Sans, Liberation Sans, Arial, Helvetica, sans&quot;,fontsize=10,height=0.25,shape=box,style=&quot;setlinewidth(0.5)&quot;,target=&quot;_top&quot;,tooltip=&quot;This class summarizes the fit of a linear regression model.&quot;];
  &quot;models.model.LikelihoodModelResults&quot; -&gt; &quot;models.regression.RegressionResults&quot; [arrowsize=0.5,style=&quot;setlinewidth(0.5)&quot;];
  &quot;models.regression.WLSModel&quot; [URL=&quot;#nipy.algorithms.statistics.models.regression.WLSModel&quot;,fontname=&quot;Vera Sans, DejaVu Sans, Liberation Sans, Arial, Helvetica, sans&quot;,fontsize=10,height=0.25,shape=box,style=&quot;setlinewidth(0.5)&quot;,target=&quot;_top&quot;,tooltip=&quot;A regression model with diagonal but non-identity covariance structure.&quot;];
  &quot;models.regression.OLSModel&quot; -&gt; &quot;models.regression.WLSModel&quot; [arrowsize=0.5,style=&quot;setlinewidth(0.5)&quot;];
}
<span class="target" id="module-nipy.algorithms.statistics.models.regression"></span><p>This module implements some standard regression models: OLS and WLS
models, as well as an AR(p) regression model.</p>
<p>Models are specified with a design matrix and are fit using their
‘fit’ method.</p>
<p>Subclasses that have more complicated covariance matrices
should write over the ‘whiten’ method as the fit method
prewhitens the response by calling ‘whiten’.</p>
<p>General reference for regression models:</p>
<dl class="simple">
<dt>‘Introduction to Linear Regression Analysis’, Douglas C. Montgomery,</dt><dd><p>Elizabeth A. Peck, G. Geoffrey Vining. Wiley, 2006.</p>
</dd>
</dl>
</div>
<div class="section" id="classes">
<h2>Classes<a class="headerlink" href="#classes" title="Permalink to this headline">¶</a></h2>
<div class="section" id="arestimator">
<h3><a class="reference internal" href="#nipy.algorithms.statistics.models.regression.AREstimator" title="nipy.algorithms.statistics.models.regression.AREstimator"><code class="xref py py-class docutils literal notranslate"><span class="pre">AREstimator</span></code></a><a class="headerlink" href="#arestimator" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="nipy.algorithms.statistics.models.regression.AREstimator">
<em class="property">class </em><code class="sig-prename descclassname">nipy.algorithms.statistics.models.regression.</code><code class="sig-name descname">AREstimator</code><span class="sig-paren">(</span><em class="sig-param">model</em>, <em class="sig-param">p=1</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nipy/algorithms/statistics/models/regression.html#AREstimator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.AREstimator" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>A class to estimate AR(p) coefficients from residuals</p>
<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.AREstimator.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">model</em>, <em class="sig-param">p=1</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nipy/algorithms/statistics/models/regression.html#AREstimator.__init__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.AREstimator.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Bias-correcting AR estimation class</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>model</strong> : <code class="docutils literal notranslate"><span class="pre">OSLModel</span></code> instance</p>
<blockquote>
<div><p>A models.regression.OLSmodel instance,
where <cite>model</cite> has attribute <code class="docutils literal notranslate"><span class="pre">design</span></code></p>
</div></blockquote>
<p><strong>p</strong> : int, optional</p>
<blockquote>
<div><p>Order of AR(p) noise</p>
</div></blockquote>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="armodel">
<h3><a class="reference internal" href="#nipy.algorithms.statistics.models.regression.ARModel" title="nipy.algorithms.statistics.models.regression.ARModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">ARModel</span></code></a><a class="headerlink" href="#armodel" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="nipy.algorithms.statistics.models.regression.ARModel">
<em class="property">class </em><code class="sig-prename descclassname">nipy.algorithms.statistics.models.regression.</code><code class="sig-name descname">ARModel</code><span class="sig-paren">(</span><em class="sig-param">design</em>, <em class="sig-param">rho</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nipy/algorithms/statistics/models/regression.html#ARModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.ARModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nipy.algorithms.statistics.models.regression.OLSModel" title="nipy.algorithms.statistics.models.regression.OLSModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">nipy.algorithms.statistics.models.regression.OLSModel</span></code></a></p>
<p>A regression model with an AR(p) covariance structure.</p>
<p>In terms of a LikelihoodModel, the parameters
are beta, the usual regression parameters,
and sigma, a scalar nuisance parameter that
shows up as multiplier in front of the AR(p) covariance.</p>
<dl class="simple">
<dt>The linear autoregressive process of order p–AR(p)–is defined as:</dt><dd><p>TODO</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nipy.algorithms.statistics.api</span> <span class="k">import</span> <span class="n">Term</span><span class="p">,</span> <span class="n">Formula</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">rec</span><span class="o">.</span><span class="n">fromarrays</span><span class="p">(([</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">9</span><span class="p">],</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">8</span><span class="p">)),</span>
<span class="gp">... </span>                         <span class="n">names</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;Y&#39;</span><span class="p">,</span> <span class="s1">&#39;X&#39;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f</span> <span class="o">=</span> <span class="n">Formula</span><span class="p">([</span><span class="n">Term</span><span class="p">(</span><span class="s2">&quot;X&quot;</span><span class="p">),</span> <span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dmtx</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">design</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">return_float</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">ARModel</span><span class="p">(</span><span class="n">dmtx</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<p>We go through the <code class="docutils literal notranslate"><span class="pre">model.iterative_fit</span></code> procedure long-hand:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">):</span>
<span class="gp">... </span>    <span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;Y&#39;</span><span class="p">])</span>
<span class="gp">... </span>    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;AR coefficients:&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">rho</span><span class="p">)</span>
<span class="gp">... </span>    <span class="n">rho</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="n">yule_walker</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;Y&quot;</span><span class="p">]</span> <span class="o">-</span> <span class="n">results</span><span class="o">.</span><span class="n">predicted</span><span class="p">,</span>
<span class="gp">... </span>                             <span class="n">order</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="gp">... </span>                             <span class="n">df</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">df_resid</span><span class="p">)</span>
<span class="gp">... </span>    <span class="n">model</span> <span class="o">=</span> <span class="n">ARModel</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">design</span><span class="p">,</span> <span class="n">rho</span><span class="p">)</span> 
<span class="gp">...</span>
<span class="go">AR coefficients: [ 0.  0.]</span>
<span class="go">AR coefficients: [-0.61530877 -1.01542645]</span>
<span class="go">AR coefficients: [-0.72660832 -1.06201457]</span>
<span class="go">AR coefficients: [-0.7220361  -1.05365352]</span>
<span class="go">AR coefficients: [-0.72229201 -1.05408193]</span>
<span class="go">AR coefficients: [-0.722278   -1.05405838]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">results</span><span class="o">.</span><span class="n">theta</span> 
<span class="go">array([ 1.59564228, -0.58562172])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">results</span><span class="o">.</span><span class="n">t</span><span class="p">()</span> 
<span class="go">array([ 38.0890515 ,  -3.45429252])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">Tcontrast</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]))</span>  
<span class="go">&lt;T contrast: effect=-0.58562172384377043, sd=0.16953449108110835,</span>
<span class="go">t=-3.4542925165805847, df_den=5&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">Fcontrast</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="mi">2</span><span class="p">)))</span>  
<span class="go">&lt;F contrast: F=4216.810299725842, df_den=5, df_num=2&gt;</span>
</pre></div>
</div>
<p>Reinitialize the model, and do the automated iterative fit</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">rho</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">iterative_fit</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;Y&#39;</span><span class="p">],</span> <span class="n">niter</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">rho</span><span class="p">)</span>  
<span class="go">[-0.7220361  -1.05365352]</span>
</pre></div>
</div>
<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.ARModel.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">design</em>, <em class="sig-param">rho</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nipy/algorithms/statistics/models/regression.html#ARModel.__init__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.ARModel.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize AR model instance</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>design</strong> : ndarray</p>
<blockquote>
<div><p>2D array with design matrix</p>
</div></blockquote>
<p><strong>rho</strong> : int or array-like</p>
<blockquote>
<div><p>If int, gives order of model, and initializes rho to zeros.  If
ndarray, gives initial estimate of rho. Be careful as <code class="docutils literal notranslate"><span class="pre">ARModel(X,</span>
<span class="pre">1)</span> <span class="pre">!=</span> <span class="pre">ARModel(X,</span> <span class="pre">1.0)</span></code>.</p>
</div></blockquote>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.ARModel.iterative_fit">
<code class="sig-name descname">iterative_fit</code><span class="sig-paren">(</span><em class="sig-param">Y</em>, <em class="sig-param">niter=3</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nipy/algorithms/statistics/models/regression.html#ARModel.iterative_fit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.ARModel.iterative_fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform an iterative two-stage procedure to estimate AR(p)
parameters and regression coefficients simultaneously.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>Y</strong> : ndarray</p>
<blockquote>
<div><p>data to which to fit model</p>
</div></blockquote>
<p><strong>niter</strong> : optional, int</p>
<blockquote>
<div><p>the number of iterations (default 3)</p>
</div></blockquote>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>None</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.ARModel.whiten">
<code class="sig-name descname">whiten</code><span class="sig-paren">(</span><em class="sig-param">X</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nipy/algorithms/statistics/models/regression.html#ARModel.whiten"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.ARModel.whiten" title="Permalink to this definition">¶</a></dt>
<dd><p>Whiten a series of columns according to AR(p) covariance structure</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> : array-like of shape (n_features)</p>
<blockquote>
<div><p>array to whiten</p>
</div></blockquote>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>wX</strong> : ndarray</p>
<blockquote>
<div><p>X whitened with order self.order AR</p>
</div></blockquote>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.ARModel.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param">Y</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.ARModel.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit model to data <cite>Y</cite></p>
<p>Full fit of the model including estimate of covariance matrix,
(whitened) residuals and scale.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>Y</strong> : array-like</p>
<blockquote>
<div><p>The dependent variable for the Least Squares problem.</p>
</div></blockquote>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>fit</strong> : RegressionResults</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.ARModel.has_intercept">
<code class="sig-name descname">has_intercept</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.ARModel.has_intercept" title="Permalink to this definition">¶</a></dt>
<dd><p>Check if column of 1s is in column space of design</p>
</dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.ARModel.information">
<code class="sig-name descname">information</code><span class="sig-paren">(</span><em class="sig-param">beta</em>, <em class="sig-param">nuisance=None</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.ARModel.information" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the information matrix at (beta, Y, nuisance).</p>
<p>See logL for details.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>beta</strong> : ndarray</p>
<blockquote>
<div><p>The parameter estimates.  Must be of length df_model.</p>
</div></blockquote>
<p><strong>nuisance</strong> : dict</p>
<blockquote>
<div><p>A dict with key ‘sigma’, which is an estimate of sigma. If None,
defaults to its maximum likelihood estimate (with beta fixed) as
<code class="docutils literal notranslate"><span class="pre">sum((Y</span> <span class="pre">-</span> <span class="pre">X*beta)**2)</span> <span class="pre">/</span> <span class="pre">n</span></code> where n=Y.shape[0], X=self.design.</p>
</div></blockquote>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>info</strong> : array</p>
<blockquote>
<div><p>The information matrix, the negative of the inverse of the Hessian
of the of the log-likelihood function evaluated at (theta, Y,
nuisance).</p>
</div></blockquote>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.ARModel.initialize">
<code class="sig-name descname">initialize</code><span class="sig-paren">(</span><em class="sig-param">design</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.ARModel.initialize" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize (possibly re-initialize) a Model instance.</p>
<p>For instance, the design matrix of a linear model may change and some
things must be recomputed.</p>
</dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.ARModel.logL">
<code class="sig-name descname">logL</code><span class="sig-paren">(</span><em class="sig-param">beta</em>, <em class="sig-param">Y</em>, <em class="sig-param">nuisance=None</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.ARModel.logL" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the value of the loglikelihood function at beta.</p>
<p>Given the whitened design matrix, the loglikelihood is evaluated
at the parameter vector, beta, for the dependent variable, Y
and the nuisance parameter, sigma.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>beta</strong> : ndarray</p>
<blockquote>
<div><p>The parameter estimates.  Must be of length df_model.</p>
</div></blockquote>
<p><strong>Y</strong> : ndarray</p>
<blockquote>
<div><p>The dependent variable</p>
</div></blockquote>
<p><strong>nuisance</strong> : dict, optional</p>
<blockquote>
<div><p>A dict with key ‘sigma’, which is an optional estimate of sigma. If
None, defaults to its maximum likelihood estimate (with beta fixed)
as <code class="docutils literal notranslate"><span class="pre">sum((Y</span> <span class="pre">-</span> <span class="pre">X*beta)**2)</span> <span class="pre">/</span> <span class="pre">n</span></code>, where n=Y.shape[0], X=self.design.</p>
</div></blockquote>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>loglf</strong> : float</p>
<blockquote>
<div><p>The value of the loglikelihood function.</p>
</div></blockquote>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>The log-Likelihood Function is defined as</p>
<div class="math notranslate nohighlight">
\[\ell(\beta,\sigma,Y)=
-\frac{n}{2}\log(2\pi\sigma^2) - \|Y-X\beta\|^2/(2\sigma^2)\]</div>
<p>The parameter <span class="math notranslate nohighlight">\(\sigma\)</span> above is what is sometimes referred to as a
nuisance parameter. That is, the likelihood is considered as a function
of <span class="math notranslate nohighlight">\(\beta\)</span>, but to evaluate it, a value of <span class="math notranslate nohighlight">\(\sigma\)</span> is
needed.</p>
<p>If <span class="math notranslate nohighlight">\(\sigma\)</span> is not provided, then its maximum likelihood estimate:</p>
<div class="math notranslate nohighlight">
\[\hat{\sigma}(\beta) = \frac{\text{SSE}(\beta)}{n}\]</div>
<p>is plugged in. This likelihood is now a function of only <span class="math notranslate nohighlight">\(\beta\)</span>
and is technically referred to as a profile-likelihood.</p>
<p class="rubric">References</p>
<dl class="citation">
<dt class="label" id="r2"><span class="brackets"><a class="fn-backref" href="#id1">R2</a></span></dt>
<dd><ol class="upperalpha simple" start="23">
<li><p>Green.  “Econometric Analysis,” 5th ed., Pearson, 2003.</p></li>
</ol>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.ARModel.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param">design=None</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.ARModel.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>After a model has been fit, results are (assumed to be) stored
in self.results, which itself should have a predict method.</p>
</dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.ARModel.rank">
<code class="sig-name descname">rank</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.ARModel.rank" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute rank of design matrix</p>
</dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.ARModel.score">
<code class="sig-name descname">score</code><span class="sig-paren">(</span><em class="sig-param">beta</em>, <em class="sig-param">Y</em>, <em class="sig-param">nuisance=None</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.ARModel.score" title="Permalink to this definition">¶</a></dt>
<dd><p>Gradient of the loglikelihood function at (beta, Y, nuisance).</p>
<p>The graient of the loglikelihood function at (beta, Y, nuisance) is the
score function.</p>
<p>See <a class="reference internal" href="#nipy.algorithms.statistics.models.regression.ARModel.logL" title="nipy.algorithms.statistics.models.regression.ARModel.logL"><code class="xref py py-meth docutils literal notranslate"><span class="pre">logL()</span></code></a> for details.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>beta</strong> : ndarray</p>
<blockquote>
<div><p>The parameter estimates.  Must be of length df_model.</p>
</div></blockquote>
<p><strong>Y</strong> : ndarray</p>
<blockquote>
<div><p>The dependent variable.</p>
</div></blockquote>
<p><strong>nuisance</strong> : dict, optional</p>
<blockquote>
<div><p>A dict with key ‘sigma’, which is an optional estimate of sigma. If
None, defaults to its maximum likelihood estimate (with beta fixed)
as <code class="docutils literal notranslate"><span class="pre">sum((Y</span> <span class="pre">-</span> <span class="pre">X*beta)**2)</span> <span class="pre">/</span> <span class="pre">n</span></code>, where n=Y.shape[0], X=self.design.</p>
</div></blockquote>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The gradient of the loglikelihood function.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="glsmodel">
<h3><a class="reference internal" href="#nipy.algorithms.statistics.models.regression.GLSModel" title="nipy.algorithms.statistics.models.regression.GLSModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">GLSModel</span></code></a><a class="headerlink" href="#glsmodel" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="nipy.algorithms.statistics.models.regression.GLSModel">
<em class="property">class </em><code class="sig-prename descclassname">nipy.algorithms.statistics.models.regression.</code><code class="sig-name descname">GLSModel</code><span class="sig-paren">(</span><em class="sig-param">design</em>, <em class="sig-param">sigma</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nipy/algorithms/statistics/models/regression.html#GLSModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.GLSModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nipy.algorithms.statistics.models.regression.OLSModel" title="nipy.algorithms.statistics.models.regression.OLSModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">nipy.algorithms.statistics.models.regression.OLSModel</span></code></a></p>
<p>Generalized least squares model with a general covariance structure</p>
<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.GLSModel.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">design</em>, <em class="sig-param">sigma</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nipy/algorithms/statistics/models/regression.html#GLSModel.__init__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.GLSModel.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>design</strong> : array-like</p>
<blockquote>
<div><p>This is your design matrix.
Data are assumed to be column ordered with
observations in rows.</p>
</div></blockquote>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.GLSModel.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param">Y</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.GLSModel.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit model to data <cite>Y</cite></p>
<p>Full fit of the model including estimate of covariance matrix,
(whitened) residuals and scale.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>Y</strong> : array-like</p>
<blockquote>
<div><p>The dependent variable for the Least Squares problem.</p>
</div></blockquote>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>fit</strong> : RegressionResults</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.GLSModel.has_intercept">
<code class="sig-name descname">has_intercept</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.GLSModel.has_intercept" title="Permalink to this definition">¶</a></dt>
<dd><p>Check if column of 1s is in column space of design</p>
</dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.GLSModel.information">
<code class="sig-name descname">information</code><span class="sig-paren">(</span><em class="sig-param">beta</em>, <em class="sig-param">nuisance=None</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.GLSModel.information" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the information matrix at (beta, Y, nuisance).</p>
<p>See logL for details.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>beta</strong> : ndarray</p>
<blockquote>
<div><p>The parameter estimates.  Must be of length df_model.</p>
</div></blockquote>
<p><strong>nuisance</strong> : dict</p>
<blockquote>
<div><p>A dict with key ‘sigma’, which is an estimate of sigma. If None,
defaults to its maximum likelihood estimate (with beta fixed) as
<code class="docutils literal notranslate"><span class="pre">sum((Y</span> <span class="pre">-</span> <span class="pre">X*beta)**2)</span> <span class="pre">/</span> <span class="pre">n</span></code> where n=Y.shape[0], X=self.design.</p>
</div></blockquote>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>info</strong> : array</p>
<blockquote>
<div><p>The information matrix, the negative of the inverse of the Hessian
of the of the log-likelihood function evaluated at (theta, Y,
nuisance).</p>
</div></blockquote>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.GLSModel.initialize">
<code class="sig-name descname">initialize</code><span class="sig-paren">(</span><em class="sig-param">design</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.GLSModel.initialize" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize (possibly re-initialize) a Model instance.</p>
<p>For instance, the design matrix of a linear model may change and some
things must be recomputed.</p>
</dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.GLSModel.logL">
<code class="sig-name descname">logL</code><span class="sig-paren">(</span><em class="sig-param">beta</em>, <em class="sig-param">Y</em>, <em class="sig-param">nuisance=None</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.GLSModel.logL" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the value of the loglikelihood function at beta.</p>
<p>Given the whitened design matrix, the loglikelihood is evaluated
at the parameter vector, beta, for the dependent variable, Y
and the nuisance parameter, sigma.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>beta</strong> : ndarray</p>
<blockquote>
<div><p>The parameter estimates.  Must be of length df_model.</p>
</div></blockquote>
<p><strong>Y</strong> : ndarray</p>
<blockquote>
<div><p>The dependent variable</p>
</div></blockquote>
<p><strong>nuisance</strong> : dict, optional</p>
<blockquote>
<div><p>A dict with key ‘sigma’, which is an optional estimate of sigma. If
None, defaults to its maximum likelihood estimate (with beta fixed)
as <code class="docutils literal notranslate"><span class="pre">sum((Y</span> <span class="pre">-</span> <span class="pre">X*beta)**2)</span> <span class="pre">/</span> <span class="pre">n</span></code>, where n=Y.shape[0], X=self.design.</p>
</div></blockquote>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>loglf</strong> : float</p>
<blockquote>
<div><p>The value of the loglikelihood function.</p>
</div></blockquote>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>The log-Likelihood Function is defined as</p>
<div class="math notranslate nohighlight">
\[\ell(\beta,\sigma,Y)=
-\frac{n}{2}\log(2\pi\sigma^2) - \|Y-X\beta\|^2/(2\sigma^2)\]</div>
<p>The parameter <span class="math notranslate nohighlight">\(\sigma\)</span> above is what is sometimes referred to as a
nuisance parameter. That is, the likelihood is considered as a function
of <span class="math notranslate nohighlight">\(\beta\)</span>, but to evaluate it, a value of <span class="math notranslate nohighlight">\(\sigma\)</span> is
needed.</p>
<p>If <span class="math notranslate nohighlight">\(\sigma\)</span> is not provided, then its maximum likelihood estimate:</p>
<div class="math notranslate nohighlight">
\[\hat{\sigma}(\beta) = \frac{\text{SSE}(\beta)}{n}\]</div>
<p>is plugged in. This likelihood is now a function of only <span class="math notranslate nohighlight">\(\beta\)</span>
and is technically referred to as a profile-likelihood.</p>
<p class="rubric">References</p>
<dl class="citation">
<dt class="label" id="r3"><span class="brackets"><a class="fn-backref" href="#id2">R3</a></span></dt>
<dd><ol class="upperalpha simple" start="23">
<li><p>Green.  “Econometric Analysis,” 5th ed., Pearson, 2003.</p></li>
</ol>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.GLSModel.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param">design=None</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.GLSModel.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>After a model has been fit, results are (assumed to be) stored
in self.results, which itself should have a predict method.</p>
</dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.GLSModel.rank">
<code class="sig-name descname">rank</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.GLSModel.rank" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute rank of design matrix</p>
</dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.GLSModel.score">
<code class="sig-name descname">score</code><span class="sig-paren">(</span><em class="sig-param">beta</em>, <em class="sig-param">Y</em>, <em class="sig-param">nuisance=None</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.GLSModel.score" title="Permalink to this definition">¶</a></dt>
<dd><p>Gradient of the loglikelihood function at (beta, Y, nuisance).</p>
<p>The graient of the loglikelihood function at (beta, Y, nuisance) is the
score function.</p>
<p>See <a class="reference internal" href="#nipy.algorithms.statistics.models.regression.GLSModel.logL" title="nipy.algorithms.statistics.models.regression.GLSModel.logL"><code class="xref py py-meth docutils literal notranslate"><span class="pre">logL()</span></code></a> for details.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>beta</strong> : ndarray</p>
<blockquote>
<div><p>The parameter estimates.  Must be of length df_model.</p>
</div></blockquote>
<p><strong>Y</strong> : ndarray</p>
<blockquote>
<div><p>The dependent variable.</p>
</div></blockquote>
<p><strong>nuisance</strong> : dict, optional</p>
<blockquote>
<div><p>A dict with key ‘sigma’, which is an optional estimate of sigma. If
None, defaults to its maximum likelihood estimate (with beta fixed)
as <code class="docutils literal notranslate"><span class="pre">sum((Y</span> <span class="pre">-</span> <span class="pre">X*beta)**2)</span> <span class="pre">/</span> <span class="pre">n</span></code>, where n=Y.shape[0], X=self.design.</p>
</div></blockquote>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The gradient of the loglikelihood function.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.GLSModel.whiten">
<code class="sig-name descname">whiten</code><span class="sig-paren">(</span><em class="sig-param">Y</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nipy/algorithms/statistics/models/regression.html#GLSModel.whiten"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.GLSModel.whiten" title="Permalink to this definition">¶</a></dt>
<dd><p>Whiten design matrix</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> : array</p>
<blockquote>
<div><p>design matrix</p>
</div></blockquote>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>wX</strong> : array</p>
<blockquote>
<div><p>This matrix is the matrix whose pseudoinverse is ultimately
used in estimating the coefficients. For OLSModel, it is
does nothing. For WLSmodel, ARmodel, it pre-applies
a square root of the covariance matrix to X.</p>
</div></blockquote>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="olsmodel">
<h3><a class="reference internal" href="#nipy.algorithms.statistics.models.regression.OLSModel" title="nipy.algorithms.statistics.models.regression.OLSModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">OLSModel</span></code></a><a class="headerlink" href="#olsmodel" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="nipy.algorithms.statistics.models.regression.OLSModel">
<em class="property">class </em><code class="sig-prename descclassname">nipy.algorithms.statistics.models.regression.</code><code class="sig-name descname">OLSModel</code><span class="sig-paren">(</span><em class="sig-param">design</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nipy/algorithms/statistics/models/regression.html#OLSModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.OLSModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="nipy.algorithms.statistics.models.model.html#nipy.algorithms.statistics.models.model.LikelihoodModel" title="nipy.algorithms.statistics.models.model.LikelihoodModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">nipy.algorithms.statistics.models.model.LikelihoodModel</span></code></a></p>
<p>A simple ordinary least squares model.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>design</strong> : array-like</p>
<blockquote>
<div><p>This is your design matrix.  Data are assumed to be column ordered with
observations in rows.</p>
</div></blockquote>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nipy.algorithms.statistics.api</span> <span class="k">import</span> <span class="n">Term</span><span class="p">,</span> <span class="n">Formula</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">rec</span><span class="o">.</span><span class="n">fromarrays</span><span class="p">(([</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">],</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">8</span><span class="p">)),</span>
<span class="gp">... </span>                         <span class="n">names</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;Y&#39;</span><span class="p">,</span> <span class="s1">&#39;X&#39;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f</span> <span class="o">=</span> <span class="n">Formula</span><span class="p">([</span><span class="n">Term</span><span class="p">(</span><span class="s2">&quot;X&quot;</span><span class="p">),</span> <span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dmtx</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">design</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">return_float</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">OLSModel</span><span class="p">(</span><span class="n">dmtx</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;Y&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">results</span><span class="o">.</span><span class="n">theta</span>
<span class="go">array([ 0.25      ,  2.14285714])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">results</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>
<span class="go">array([ 0.98019606,  1.87867287])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">Tcontrast</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]))</span>  
<span class="go">&lt;T contrast: effect=2.14285714286, sd=1.14062281591, t=1.87867287326,</span>
<span class="go">df_den=5&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">Fcontrast</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">2</span><span class="p">)))</span>  
<span class="go">&lt;F contrast: F=19.4607843137, df_den=5, df_num=2&gt;</span>
</pre></div>
</div>
<p class="rubric">Attributes</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 12%" />
<col style="width: 88%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>design</strong></p></td>
<td><p>(ndarray) This is the design, or X, matrix.</p></td>
</tr>
<tr class="row-even"><td><p><strong>wdesign</strong></p></td>
<td><p>(ndarray) This is the whitened design matrix.  <cite>design</cite> == <cite>wdesign</cite> by default for the OLSModel, though models that inherit from the OLSModel will whiten the design.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>calc_beta</strong></p></td>
<td><p>(ndarray) This is the Moore-Penrose pseudoinverse of the whitened design matrix.</p></td>
</tr>
<tr class="row-even"><td><p><strong>normalized_cov_beta</strong></p></td>
<td><p>(ndarray) <code class="docutils literal notranslate"><span class="pre">np.dot(calc_beta,</span> <span class="pre">calc_beta.T)</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><strong>df_resid</strong></p></td>
<td><p>(scalar) Degrees of freedom of the residuals.  Number of observations less the rank of the design.</p></td>
</tr>
<tr class="row-even"><td><p><strong>df_model</strong></p></td>
<td><p>(scalar) Degrees of freedome of the model.  The rank of the design.</p></td>
</tr>
</tbody>
</table>
<p class="rubric">Methods</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 75%" />
<col style="width: 25%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>model.__init___(design)</strong></p></td>
<td></td>
</tr>
<tr class="row-even"><td><p><strong>model.logL(b=self.beta, Y)</strong></p></td>
<td></td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.OLSModel.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">design</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nipy/algorithms/statistics/models/regression.html#OLSModel.__init__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.OLSModel.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>design</strong> : array-like</p>
<blockquote>
<div><p>This is your design matrix.
Data are assumed to be column ordered with
observations in rows.</p>
</div></blockquote>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.OLSModel.initialize">
<code class="sig-name descname">initialize</code><span class="sig-paren">(</span><em class="sig-param">design</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nipy/algorithms/statistics/models/regression.html#OLSModel.initialize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.OLSModel.initialize" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize (possibly re-initialize) a Model instance.</p>
<p>For instance, the design matrix of a linear model may change and some
things must be recomputed.</p>
</dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.OLSModel.logL">
<code class="sig-name descname">logL</code><span class="sig-paren">(</span><em class="sig-param">beta</em>, <em class="sig-param">Y</em>, <em class="sig-param">nuisance=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nipy/algorithms/statistics/models/regression.html#OLSModel.logL"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.OLSModel.logL" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the value of the loglikelihood function at beta.</p>
<p>Given the whitened design matrix, the loglikelihood is evaluated
at the parameter vector, beta, for the dependent variable, Y
and the nuisance parameter, sigma.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>beta</strong> : ndarray</p>
<blockquote>
<div><p>The parameter estimates.  Must be of length df_model.</p>
</div></blockquote>
<p><strong>Y</strong> : ndarray</p>
<blockquote>
<div><p>The dependent variable</p>
</div></blockquote>
<p><strong>nuisance</strong> : dict, optional</p>
<blockquote>
<div><p>A dict with key ‘sigma’, which is an optional estimate of sigma. If
None, defaults to its maximum likelihood estimate (with beta fixed)
as <code class="docutils literal notranslate"><span class="pre">sum((Y</span> <span class="pre">-</span> <span class="pre">X*beta)**2)</span> <span class="pre">/</span> <span class="pre">n</span></code>, where n=Y.shape[0], X=self.design.</p>
</div></blockquote>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>loglf</strong> : float</p>
<blockquote>
<div><p>The value of the loglikelihood function.</p>
</div></blockquote>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>The log-Likelihood Function is defined as</p>
<div class="math notranslate nohighlight">
\[\ell(\beta,\sigma,Y)=
-\frac{n}{2}\log(2\pi\sigma^2) - \|Y-X\beta\|^2/(2\sigma^2)\]</div>
<p>The parameter <span class="math notranslate nohighlight">\(\sigma\)</span> above is what is sometimes referred to as a
nuisance parameter. That is, the likelihood is considered as a function
of <span class="math notranslate nohighlight">\(\beta\)</span>, but to evaluate it, a value of <span class="math notranslate nohighlight">\(\sigma\)</span> is
needed.</p>
<p>If <span class="math notranslate nohighlight">\(\sigma\)</span> is not provided, then its maximum likelihood estimate:</p>
<div class="math notranslate nohighlight">
\[\hat{\sigma}(\beta) = \frac{\text{SSE}(\beta)}{n}\]</div>
<p>is plugged in. This likelihood is now a function of only <span class="math notranslate nohighlight">\(\beta\)</span>
and is technically referred to as a profile-likelihood.</p>
<p class="rubric">References</p>
<dl class="citation">
<dt class="label" id="r4"><span class="brackets"><a class="fn-backref" href="#id3">R4</a></span></dt>
<dd><ol class="upperalpha simple" start="23">
<li><p>Green.  “Econometric Analysis,” 5th ed., Pearson, 2003.</p></li>
</ol>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.OLSModel.score">
<code class="sig-name descname">score</code><span class="sig-paren">(</span><em class="sig-param">beta</em>, <em class="sig-param">Y</em>, <em class="sig-param">nuisance=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nipy/algorithms/statistics/models/regression.html#OLSModel.score"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.OLSModel.score" title="Permalink to this definition">¶</a></dt>
<dd><p>Gradient of the loglikelihood function at (beta, Y, nuisance).</p>
<p>The graient of the loglikelihood function at (beta, Y, nuisance) is the
score function.</p>
<p>See <a class="reference internal" href="#nipy.algorithms.statistics.models.regression.OLSModel.logL" title="nipy.algorithms.statistics.models.regression.OLSModel.logL"><code class="xref py py-meth docutils literal notranslate"><span class="pre">logL()</span></code></a> for details.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>beta</strong> : ndarray</p>
<blockquote>
<div><p>The parameter estimates.  Must be of length df_model.</p>
</div></blockquote>
<p><strong>Y</strong> : ndarray</p>
<blockquote>
<div><p>The dependent variable.</p>
</div></blockquote>
<p><strong>nuisance</strong> : dict, optional</p>
<blockquote>
<div><p>A dict with key ‘sigma’, which is an optional estimate of sigma. If
None, defaults to its maximum likelihood estimate (with beta fixed)
as <code class="docutils literal notranslate"><span class="pre">sum((Y</span> <span class="pre">-</span> <span class="pre">X*beta)**2)</span> <span class="pre">/</span> <span class="pre">n</span></code>, where n=Y.shape[0], X=self.design.</p>
</div></blockquote>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The gradient of the loglikelihood function.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.OLSModel.information">
<code class="sig-name descname">information</code><span class="sig-paren">(</span><em class="sig-param">beta</em>, <em class="sig-param">nuisance=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nipy/algorithms/statistics/models/regression.html#OLSModel.information"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.OLSModel.information" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the information matrix at (beta, Y, nuisance).</p>
<p>See logL for details.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>beta</strong> : ndarray</p>
<blockquote>
<div><p>The parameter estimates.  Must be of length df_model.</p>
</div></blockquote>
<p><strong>nuisance</strong> : dict</p>
<blockquote>
<div><p>A dict with key ‘sigma’, which is an estimate of sigma. If None,
defaults to its maximum likelihood estimate (with beta fixed) as
<code class="docutils literal notranslate"><span class="pre">sum((Y</span> <span class="pre">-</span> <span class="pre">X*beta)**2)</span> <span class="pre">/</span> <span class="pre">n</span></code> where n=Y.shape[0], X=self.design.</p>
</div></blockquote>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>info</strong> : array</p>
<blockquote>
<div><p>The information matrix, the negative of the inverse of the Hessian
of the of the log-likelihood function evaluated at (theta, Y,
nuisance).</p>
</div></blockquote>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.OLSModel.whiten">
<code class="sig-name descname">whiten</code><span class="sig-paren">(</span><em class="sig-param">X</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nipy/algorithms/statistics/models/regression.html#OLSModel.whiten"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.OLSModel.whiten" title="Permalink to this definition">¶</a></dt>
<dd><p>Whiten design matrix</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> : array</p>
<blockquote>
<div><p>design matrix</p>
</div></blockquote>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>wX</strong> : array</p>
<blockquote>
<div><p>This matrix is the matrix whose pseudoinverse is ultimately
used in estimating the coefficients. For OLSModel, it is
does nothing. For WLSmodel, ARmodel, it pre-applies
a square root of the covariance matrix to X.</p>
</div></blockquote>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.OLSModel.has_intercept">
<code class="sig-name descname">has_intercept</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nipy/algorithms/statistics/models/regression.html#OLSModel.has_intercept"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.OLSModel.has_intercept" title="Permalink to this definition">¶</a></dt>
<dd><p>Check if column of 1s is in column space of design</p>
</dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.OLSModel.rank">
<code class="sig-name descname">rank</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nipy/algorithms/statistics/models/regression.html#OLSModel.rank"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.OLSModel.rank" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute rank of design matrix</p>
</dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.OLSModel.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param">Y</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nipy/algorithms/statistics/models/regression.html#OLSModel.fit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.OLSModel.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit model to data <cite>Y</cite></p>
<p>Full fit of the model including estimate of covariance matrix,
(whitened) residuals and scale.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>Y</strong> : array-like</p>
<blockquote>
<div><p>The dependent variable for the Least Squares problem.</p>
</div></blockquote>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>fit</strong> : RegressionResults</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.OLSModel.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param">design=None</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.OLSModel.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>After a model has been fit, results are (assumed to be) stored
in self.results, which itself should have a predict method.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="regressionresults">
<h3><a class="reference internal" href="#nipy.algorithms.statistics.models.regression.RegressionResults" title="nipy.algorithms.statistics.models.regression.RegressionResults"><code class="xref py py-class docutils literal notranslate"><span class="pre">RegressionResults</span></code></a><a class="headerlink" href="#regressionresults" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="nipy.algorithms.statistics.models.regression.RegressionResults">
<em class="property">class </em><code class="sig-prename descclassname">nipy.algorithms.statistics.models.regression.</code><code class="sig-name descname">RegressionResults</code><span class="sig-paren">(</span><em class="sig-param">theta</em>, <em class="sig-param">Y</em>, <em class="sig-param">model</em>, <em class="sig-param">wY</em>, <em class="sig-param">wresid</em>, <em class="sig-param">cov=None</em>, <em class="sig-param">dispersion=1.0</em>, <em class="sig-param">nuisance=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nipy/algorithms/statistics/models/regression.html#RegressionResults"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.RegressionResults" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="nipy.algorithms.statistics.models.model.html#nipy.algorithms.statistics.models.model.LikelihoodModelResults" title="nipy.algorithms.statistics.models.model.LikelihoodModelResults"><code class="xref py py-class docutils literal notranslate"><span class="pre">nipy.algorithms.statistics.models.model.LikelihoodModelResults</span></code></a></p>
<p>This class summarizes the fit of a linear regression model.</p>
<p>It handles the output of contrasts, estimates of covariance, etc.</p>
<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.RegressionResults.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">theta</em>, <em class="sig-param">Y</em>, <em class="sig-param">model</em>, <em class="sig-param">wY</em>, <em class="sig-param">wresid</em>, <em class="sig-param">cov=None</em>, <em class="sig-param">dispersion=1.0</em>, <em class="sig-param">nuisance=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nipy/algorithms/statistics/models/regression.html#RegressionResults.__init__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.RegressionResults.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>See LikelihoodModelResults constructor.</p>
<p>The only difference is that the whitened Y and residual values
are stored for a regression model.</p>
</dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.RegressionResults.resid">
<code class="sig-name descname">resid</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nipy/algorithms/statistics/models/regression.html#RegressionResults.resid"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.RegressionResults.resid" title="Permalink to this definition">¶</a></dt>
<dd><p>Residuals from the fit.</p>
</dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.RegressionResults.norm_resid">
<code class="sig-name descname">norm_resid</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nipy/algorithms/statistics/models/regression.html#RegressionResults.norm_resid"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.RegressionResults.norm_resid" title="Permalink to this definition">¶</a></dt>
<dd><p>Residuals, normalized to have unit length.</p>
<p class="rubric">Notes</p>
<p>Is this supposed to return “stanardized residuals,”
residuals standardized
to have mean zero and approximately unit variance?</p>
<p>d_i = e_i / sqrt(MS_E)</p>
<p>Where MS_E = SSE / (n - k)</p>
<dl class="simple">
<dt>See: Montgomery and Peck 3.2.1 p. 68</dt><dd><p>Davidson and MacKinnon 15.2 p 662</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.RegressionResults.predicted">
<code class="sig-name descname">predicted</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nipy/algorithms/statistics/models/regression.html#RegressionResults.predicted"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.RegressionResults.predicted" title="Permalink to this definition">¶</a></dt>
<dd><p>Return linear predictor values from a design matrix.</p>
</dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.RegressionResults.R2_adj">
<code class="sig-name descname">R2_adj</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nipy/algorithms/statistics/models/regression.html#RegressionResults.R2_adj"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.RegressionResults.R2_adj" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the R^2 value for each row of the response Y.</p>
<p class="rubric">Notes</p>
<p>Changed to the textbook definition of R^2.</p>
<p>See: Davidson and MacKinnon p 74</p>
</dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.RegressionResults.R2">
<code class="sig-name descname">R2</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nipy/algorithms/statistics/models/regression.html#RegressionResults.R2"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.RegressionResults.R2" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the adjusted R^2 value for each row of the response Y.</p>
<p class="rubric">Notes</p>
<p>Changed to the textbook definition of R^2.</p>
<p>See: Davidson and MacKinnon p 74</p>
</dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.RegressionResults.SST">
<code class="sig-name descname">SST</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nipy/algorithms/statistics/models/regression.html#RegressionResults.SST"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.RegressionResults.SST" title="Permalink to this definition">¶</a></dt>
<dd><p>Total sum of squares. If not from an OLS model this is “pseudo”-SST.</p>
</dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.RegressionResults.SSE">
<code class="sig-name descname">SSE</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nipy/algorithms/statistics/models/regression.html#RegressionResults.SSE"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.RegressionResults.SSE" title="Permalink to this definition">¶</a></dt>
<dd><p>Error sum of squares. If not from an OLS model this is “pseudo”-SSE.</p>
</dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.RegressionResults.SSR">
<code class="sig-name descname">SSR</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nipy/algorithms/statistics/models/regression.html#RegressionResults.SSR"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.RegressionResults.SSR" title="Permalink to this definition">¶</a></dt>
<dd><p>Regression sum of squares</p>
</dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.RegressionResults.MSR">
<code class="sig-name descname">MSR</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nipy/algorithms/statistics/models/regression.html#RegressionResults.MSR"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.RegressionResults.MSR" title="Permalink to this definition">¶</a></dt>
<dd><p>Mean square (regression)</p>
</dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.RegressionResults.MSE">
<code class="sig-name descname">MSE</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nipy/algorithms/statistics/models/regression.html#RegressionResults.MSE"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.RegressionResults.MSE" title="Permalink to this definition">¶</a></dt>
<dd><p>Mean square (error)</p>
</dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.RegressionResults.MST">
<code class="sig-name descname">MST</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nipy/algorithms/statistics/models/regression.html#RegressionResults.MST"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.RegressionResults.MST" title="Permalink to this definition">¶</a></dt>
<dd><p>Mean square (total)</p>
</dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.RegressionResults.F_overall">
<code class="sig-name descname">F_overall</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nipy/algorithms/statistics/models/regression.html#RegressionResults.F_overall"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.RegressionResults.F_overall" title="Permalink to this definition">¶</a></dt>
<dd><p>Overall goodness of fit F test,
comparing model to a model with just an intercept.
If not an OLS model this is a pseudo-F.</p>
</dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.RegressionResults.AIC">
<code class="sig-name descname">AIC</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.RegressionResults.AIC" title="Permalink to this definition">¶</a></dt>
<dd><p>Akaike Information Criterion</p>
</dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.RegressionResults.BIC">
<code class="sig-name descname">BIC</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.RegressionResults.BIC" title="Permalink to this definition">¶</a></dt>
<dd><p>Schwarz’s Bayesian Information Criterion</p>
</dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.RegressionResults.Fcontrast">
<code class="sig-name descname">Fcontrast</code><span class="sig-paren">(</span><em class="sig-param">matrix</em>, <em class="sig-param">dispersion=None</em>, <em class="sig-param">invcov=None</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.RegressionResults.Fcontrast" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute an Fcontrast for a contrast matrix <cite>matrix</cite>.</p>
<p>Here, <cite>matrix</cite> M is assumed to be non-singular. More precisely</p>
<div class="math notranslate nohighlight">
\[M pX pX' M'\]</div>
<p>is assumed invertible. Here, <span class="math notranslate nohighlight">\(pX\)</span> is the generalized inverse of
the design matrix of the model. There can be problems in non-OLS models
where the rank of the covariance of the noise is not full.</p>
<p>See the contrast module to see how to specify contrasts.  In particular,
the matrices from these contrasts will always be non-singular in the
sense above.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>matrix</strong> : 1D array-like</p>
<blockquote>
<div><p>contrast matrix</p>
</div></blockquote>
<p><strong>dispersion</strong> : None or float, optional</p>
<blockquote>
<div><p>If None, use <code class="docutils literal notranslate"><span class="pre">self.dispersion</span></code></p>
</div></blockquote>
<p><strong>invcov</strong> : None or array, optional</p>
<blockquote>
<div><p>Known inverse of variance covariance matrix.
If None, calculate this matrix.</p>
</div></blockquote>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>f_res</strong> : <code class="docutils literal notranslate"><span class="pre">FContrastResults</span></code> instance</p>
<blockquote>
<div><p>with attributes F, df_den, df_num</p>
</div></blockquote>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>For F contrasts, we now specify an effect and covariance</p>
</dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.RegressionResults.Tcontrast">
<code class="sig-name descname">Tcontrast</code><span class="sig-paren">(</span><em class="sig-param">matrix</em>, <em class="sig-param">store=('t'</em>, <em class="sig-param">'effect'</em>, <em class="sig-param">'sd')</em>, <em class="sig-param">dispersion=None</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.RegressionResults.Tcontrast" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute a Tcontrast for a row vector <cite>matrix</cite></p>
<p>To get the t-statistic for a single column, use the ‘t’ method.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>matrix</strong> : 1D array-like</p>
<blockquote>
<div><p>contrast matrix</p>
</div></blockquote>
<p><strong>store</strong> : sequence, optional</p>
<blockquote>
<div><p>components of t to store in results output object.  Defaults to all
components (‘t’, ‘effect’, ‘sd’).</p>
</div></blockquote>
<p><strong>dispersion</strong> : None or float, optional</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>res</strong> : <code class="docutils literal notranslate"><span class="pre">TContrastResults</span></code> object</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.RegressionResults.conf_int">
<code class="sig-name descname">conf_int</code><span class="sig-paren">(</span><em class="sig-param">alpha=0.05</em>, <em class="sig-param">cols=None</em>, <em class="sig-param">dispersion=None</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.RegressionResults.conf_int" title="Permalink to this definition">¶</a></dt>
<dd><p>The confidence interval of the specified theta estimates.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>alpha</strong> : float, optional</p>
<blockquote>
<div><p>The <cite>alpha</cite> level for the confidence interval.
ie., <cite>alpha</cite> = .05 returns a 95% confidence interval.</p>
</div></blockquote>
<p><strong>cols</strong> : tuple, optional</p>
<blockquote>
<div><p><cite>cols</cite> specifies which confidence intervals to return</p>
</div></blockquote>
<p><strong>dispersion</strong> : None or scalar</p>
<blockquote>
<div><p>scale factor for the variance / covariance (see class docstring and
<code class="docutils literal notranslate"><span class="pre">vcov</span></code> method docstring)</p>
</div></blockquote>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>cis</strong> : ndarray</p>
<blockquote>
<div><p><cite>cis</cite> is shape <code class="docutils literal notranslate"><span class="pre">(len(cols),</span> <span class="pre">2)</span></code> where each row contains [lower,
upper] for the given entry in <cite>cols</cite></p>
</div></blockquote>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>Confidence intervals are two-tailed.
TODO:
tails : string, optional</p>
<blockquote>
<div><p><cite>tails</cite> can be “two”, “upper”, or “lower”</p>
</div></blockquote>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">numpy.random</span> <span class="k">import</span> <span class="n">standard_normal</span> <span class="k">as</span> <span class="n">stan</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nipy.algorithms.statistics.models.regression</span> <span class="k">import</span> <span class="n">OLSModel</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">stan</span><span class="p">((</span><span class="mi">30</span><span class="p">,</span><span class="mi">1</span><span class="p">)),</span><span class="n">stan</span><span class="p">((</span><span class="mi">30</span><span class="p">,</span><span class="mi">1</span><span class="p">)),</span><span class="n">stan</span><span class="p">((</span><span class="mi">30</span><span class="p">,</span><span class="mi">1</span><span class="p">))))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">beta</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">3.25</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mf">7.0</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">beta</span><span class="p">)</span> <span class="o">+</span> <span class="n">stan</span><span class="p">((</span><span class="mi">30</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">OLSModel</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">confidence_intervals</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">conf_int</span><span class="p">(</span><span class="n">cols</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.RegressionResults.logL">
<code class="sig-name descname">logL</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.RegressionResults.logL" title="Permalink to this definition">¶</a></dt>
<dd><p>The maximized log-likelihood</p>
</dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.RegressionResults.t">
<code class="sig-name descname">t</code><span class="sig-paren">(</span><em class="sig-param">column=None</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.RegressionResults.t" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the (Wald) t-statistic for a given parameter estimate.</p>
<p>Use Tcontrast for more complicated (Wald) t-statistics.</p>
</dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.RegressionResults.vcov">
<code class="sig-name descname">vcov</code><span class="sig-paren">(</span><em class="sig-param">matrix=None</em>, <em class="sig-param">column=None</em>, <em class="sig-param">dispersion=None</em>, <em class="sig-param">other=None</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.RegressionResults.vcov" title="Permalink to this definition">¶</a></dt>
<dd><p>Variance/covariance matrix of linear contrast</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>matrix: (dim, self.theta.shape[0]) array, optional</strong></p>
<blockquote>
<div><p>numerical contrast specification, where <code class="docutils literal notranslate"><span class="pre">dim</span></code> refers to the
‘dimension’ of the contrast i.e. 1 for t contrasts, 1 or more
for F contrasts.</p>
</div></blockquote>
<p><strong>column: int, optional</strong></p>
<blockquote>
<div><p>alternative way of specifying contrasts (column index)</p>
</div></blockquote>
<p><strong>dispersion: float or (n_voxels,) array, optional</strong></p>
<blockquote>
<div><p>value(s) for the dispersion parameters</p>
</div></blockquote>
<p><strong>other: (dim, self.theta.shape[0]) array, optional</strong></p>
<blockquote>
<div><p>alternative contrast specification (?)</p>
</div></blockquote>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>cov: (dim, dim) or (n_voxels, dim, dim) array</p>
<blockquote>
<div><p>the estimated covariance matrix/matrices</p>
</div></blockquote>
<p>Returns the variance/covariance matrix of a linear contrast of the</p>
<p>estimates of theta, multiplied by <cite>dispersion</cite> which will often be an</p>
<p>estimate of <cite>dispersion</cite>, like, sigma^2.</p>
<p>The covariance of interest is either specified as a (set of) column(s)</p>
<p>or a matrix.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="wlsmodel">
<h3><a class="reference internal" href="#nipy.algorithms.statistics.models.regression.WLSModel" title="nipy.algorithms.statistics.models.regression.WLSModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">WLSModel</span></code></a><a class="headerlink" href="#wlsmodel" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="nipy.algorithms.statistics.models.regression.WLSModel">
<em class="property">class </em><code class="sig-prename descclassname">nipy.algorithms.statistics.models.regression.</code><code class="sig-name descname">WLSModel</code><span class="sig-paren">(</span><em class="sig-param">design</em>, <em class="sig-param">weights=1</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nipy/algorithms/statistics/models/regression.html#WLSModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.WLSModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nipy.algorithms.statistics.models.regression.OLSModel" title="nipy.algorithms.statistics.models.regression.OLSModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">nipy.algorithms.statistics.models.regression.OLSModel</span></code></a></p>
<p>A regression model with diagonal but non-identity covariance structure.</p>
<p>The weights are presumed to be (proportional to the) inverse
of the variance of the observations.</p>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nipy.algorithms.statistics.api</span> <span class="k">import</span> <span class="n">Term</span><span class="p">,</span> <span class="n">Formula</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">rec</span><span class="o">.</span><span class="n">fromarrays</span><span class="p">(([</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">],</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">8</span><span class="p">)),</span>
<span class="gp">... </span>                         <span class="n">names</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;Y&#39;</span><span class="p">,</span> <span class="s1">&#39;X&#39;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f</span> <span class="o">=</span> <span class="n">Formula</span><span class="p">([</span><span class="n">Term</span><span class="p">(</span><span class="s2">&quot;X&quot;</span><span class="p">),</span> <span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dmtx</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">design</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">return_float</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">WLSModel</span><span class="p">(</span><span class="n">dmtx</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;Y&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">results</span><span class="o">.</span><span class="n">theta</span>
<span class="go">array([ 0.0952381 ,  2.91666667])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">results</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>
<span class="go">array([ 0.35684428,  2.0652652 ])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">Tcontrast</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]))</span>  
<span class="go">&lt;T contrast: effect=2.91666666667, sd=1.41224801095, t=2.06526519708,</span>
<span class="go">df_den=5&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">Fcontrast</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="mi">2</span><span class="p">)))</span>  
<span class="go">&lt;F contrast: F=26.9986072423, df_den=5, df_num=2&gt;</span>
</pre></div>
</div>
<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.WLSModel.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">design</em>, <em class="sig-param">weights=1</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nipy/algorithms/statistics/models/regression.html#WLSModel.__init__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.WLSModel.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>design</strong> : array-like</p>
<blockquote>
<div><p>This is your design matrix.
Data are assumed to be column ordered with
observations in rows.</p>
</div></blockquote>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.WLSModel.whiten">
<code class="sig-name descname">whiten</code><span class="sig-paren">(</span><em class="sig-param">X</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nipy/algorithms/statistics/models/regression.html#WLSModel.whiten"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.WLSModel.whiten" title="Permalink to this definition">¶</a></dt>
<dd><p>Whitener for WLS model, multiplies by sqrt(self.weights)</p>
</dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.WLSModel.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param">Y</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.WLSModel.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit model to data <cite>Y</cite></p>
<p>Full fit of the model including estimate of covariance matrix,
(whitened) residuals and scale.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>Y</strong> : array-like</p>
<blockquote>
<div><p>The dependent variable for the Least Squares problem.</p>
</div></blockquote>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>fit</strong> : RegressionResults</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.WLSModel.has_intercept">
<code class="sig-name descname">has_intercept</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.WLSModel.has_intercept" title="Permalink to this definition">¶</a></dt>
<dd><p>Check if column of 1s is in column space of design</p>
</dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.WLSModel.information">
<code class="sig-name descname">information</code><span class="sig-paren">(</span><em class="sig-param">beta</em>, <em class="sig-param">nuisance=None</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.WLSModel.information" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the information matrix at (beta, Y, nuisance).</p>
<p>See logL for details.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>beta</strong> : ndarray</p>
<blockquote>
<div><p>The parameter estimates.  Must be of length df_model.</p>
</div></blockquote>
<p><strong>nuisance</strong> : dict</p>
<blockquote>
<div><p>A dict with key ‘sigma’, which is an estimate of sigma. If None,
defaults to its maximum likelihood estimate (with beta fixed) as
<code class="docutils literal notranslate"><span class="pre">sum((Y</span> <span class="pre">-</span> <span class="pre">X*beta)**2)</span> <span class="pre">/</span> <span class="pre">n</span></code> where n=Y.shape[0], X=self.design.</p>
</div></blockquote>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>info</strong> : array</p>
<blockquote>
<div><p>The information matrix, the negative of the inverse of the Hessian
of the of the log-likelihood function evaluated at (theta, Y,
nuisance).</p>
</div></blockquote>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.WLSModel.initialize">
<code class="sig-name descname">initialize</code><span class="sig-paren">(</span><em class="sig-param">design</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.WLSModel.initialize" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize (possibly re-initialize) a Model instance.</p>
<p>For instance, the design matrix of a linear model may change and some
things must be recomputed.</p>
</dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.WLSModel.logL">
<code class="sig-name descname">logL</code><span class="sig-paren">(</span><em class="sig-param">beta</em>, <em class="sig-param">Y</em>, <em class="sig-param">nuisance=None</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.WLSModel.logL" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the value of the loglikelihood function at beta.</p>
<p>Given the whitened design matrix, the loglikelihood is evaluated
at the parameter vector, beta, for the dependent variable, Y
and the nuisance parameter, sigma.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>beta</strong> : ndarray</p>
<blockquote>
<div><p>The parameter estimates.  Must be of length df_model.</p>
</div></blockquote>
<p><strong>Y</strong> : ndarray</p>
<blockquote>
<div><p>The dependent variable</p>
</div></blockquote>
<p><strong>nuisance</strong> : dict, optional</p>
<blockquote>
<div><p>A dict with key ‘sigma’, which is an optional estimate of sigma. If
None, defaults to its maximum likelihood estimate (with beta fixed)
as <code class="docutils literal notranslate"><span class="pre">sum((Y</span> <span class="pre">-</span> <span class="pre">X*beta)**2)</span> <span class="pre">/</span> <span class="pre">n</span></code>, where n=Y.shape[0], X=self.design.</p>
</div></blockquote>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>loglf</strong> : float</p>
<blockquote>
<div><p>The value of the loglikelihood function.</p>
</div></blockquote>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>The log-Likelihood Function is defined as</p>
<div class="math notranslate nohighlight">
\[\ell(\beta,\sigma,Y)=
-\frac{n}{2}\log(2\pi\sigma^2) - \|Y-X\beta\|^2/(2\sigma^2)\]</div>
<p>The parameter <span class="math notranslate nohighlight">\(\sigma\)</span> above is what is sometimes referred to as a
nuisance parameter. That is, the likelihood is considered as a function
of <span class="math notranslate nohighlight">\(\beta\)</span>, but to evaluate it, a value of <span class="math notranslate nohighlight">\(\sigma\)</span> is
needed.</p>
<p>If <span class="math notranslate nohighlight">\(\sigma\)</span> is not provided, then its maximum likelihood estimate:</p>
<div class="math notranslate nohighlight">
\[\hat{\sigma}(\beta) = \frac{\text{SSE}(\beta)}{n}\]</div>
<p>is plugged in. This likelihood is now a function of only <span class="math notranslate nohighlight">\(\beta\)</span>
and is technically referred to as a profile-likelihood.</p>
<p class="rubric">References</p>
<dl class="citation">
<dt class="label" id="r5"><span class="brackets"><a class="fn-backref" href="#id4">R5</a></span></dt>
<dd><ol class="upperalpha simple" start="23">
<li><p>Green.  “Econometric Analysis,” 5th ed., Pearson, 2003.</p></li>
</ol>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.WLSModel.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param">design=None</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.WLSModel.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>After a model has been fit, results are (assumed to be) stored
in self.results, which itself should have a predict method.</p>
</dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.WLSModel.rank">
<code class="sig-name descname">rank</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.WLSModel.rank" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute rank of design matrix</p>
</dd></dl>

<dl class="method">
<dt id="nipy.algorithms.statistics.models.regression.WLSModel.score">
<code class="sig-name descname">score</code><span class="sig-paren">(</span><em class="sig-param">beta</em>, <em class="sig-param">Y</em>, <em class="sig-param">nuisance=None</em><span class="sig-paren">)</span><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.WLSModel.score" title="Permalink to this definition">¶</a></dt>
<dd><p>Gradient of the loglikelihood function at (beta, Y, nuisance).</p>
<p>The graient of the loglikelihood function at (beta, Y, nuisance) is the
score function.</p>
<p>See <a class="reference internal" href="#nipy.algorithms.statistics.models.regression.WLSModel.logL" title="nipy.algorithms.statistics.models.regression.WLSModel.logL"><code class="xref py py-meth docutils literal notranslate"><span class="pre">logL()</span></code></a> for details.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>beta</strong> : ndarray</p>
<blockquote>
<div><p>The parameter estimates.  Must be of length df_model.</p>
</div></blockquote>
<p><strong>Y</strong> : ndarray</p>
<blockquote>
<div><p>The dependent variable.</p>
</div></blockquote>
<p><strong>nuisance</strong> : dict, optional</p>
<blockquote>
<div><p>A dict with key ‘sigma’, which is an optional estimate of sigma. If
None, defaults to its maximum likelihood estimate (with beta fixed)
as <code class="docutils literal notranslate"><span class="pre">sum((Y</span> <span class="pre">-</span> <span class="pre">X*beta)**2)</span> <span class="pre">/</span> <span class="pre">n</span></code>, where n=Y.shape[0], X=self.design.</p>
</div></blockquote>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The gradient of the loglikelihood function.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
</div>
<div class="section" id="functions">
<h2>Functions<a class="headerlink" href="#functions" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="nipy.algorithms.statistics.models.regression.ar_bias_correct">
<code class="sig-prename descclassname">nipy.algorithms.statistics.models.regression.</code><code class="sig-name descname">ar_bias_correct</code><span class="sig-paren">(</span><em class="sig-param">results</em>, <em class="sig-param">order</em>, <em class="sig-param">invM=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nipy/algorithms/statistics/models/regression.html#ar_bias_correct"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.ar_bias_correct" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply bias correction in calculating AR(p) coefficients from <cite>results</cite></p>
<p>There is a slight bias in the rho estimates on residuals due to the
correlations induced in the residuals by fitting a linear model.  See
<a class="reference internal" href="#worsley20025" id="id5"><span>[Worsley20025]</span></a>.</p>
<p>This routine implements the bias correction described in appendix A.1 of
<a class="reference internal" href="#worsley20025" id="id6"><span>[Worsley20025]</span></a>.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>results</strong> : ndarray or results object</p>
<blockquote>
<div><p>If ndarray, assume these are residuals, from a simple model.  If a
results object, with attribute <code class="docutils literal notranslate"><span class="pre">resid</span></code>, then use these for the
residuals. See Notes for more detail</p>
</div></blockquote>
<p><strong>order</strong> : int</p>
<blockquote>
<div><p>Order <code class="docutils literal notranslate"><span class="pre">p</span></code> of AR(p) model</p>
</div></blockquote>
<p><strong>invM</strong> : None or array</p>
<blockquote>
<div><p>Known bias correcting matrix for covariance.  If None, calculate from
<code class="docutils literal notranslate"><span class="pre">results.model</span></code></p>
</div></blockquote>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>rho</strong> : array</p>
<blockquote>
<div><p>Bias-corrected AR(p) coefficients</p>
</div></blockquote>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>If <cite>results</cite> has attributes <code class="docutils literal notranslate"><span class="pre">resid</span></code> and <code class="docutils literal notranslate"><span class="pre">scale</span></code>, then assume <code class="docutils literal notranslate"><span class="pre">scale</span></code>
has come from a fit of a potentially customized model, and we use that for
the sum of squared residuals.  In this case we also need
<code class="docutils literal notranslate"><span class="pre">results.df_resid</span></code>.  Otherwise we assume this is a simple Gaussian model,
like OLS, and take the simple sum of squares of the residuals.</p>
<p class="rubric">References</p>
<dl class="citation">
<dt class="label" id="worsley20025"><span class="brackets">Worsley20025</span><span class="fn-backref">(<a href="#id5">1</a>,<a href="#id6">2</a>,<a href="#id7">3</a>)</span></dt>
<dd><p>K.J. Worsley, C.H. Liao, J. Aston, V. Petre, G.H. Duncan,
F. Morales, A.C. Evans (2002) A General Statistical Analysis for fMRI
Data.  Neuroimage 15:1:15</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="nipy.algorithms.statistics.models.regression.ar_bias_corrector">
<code class="sig-prename descclassname">nipy.algorithms.statistics.models.regression.</code><code class="sig-name descname">ar_bias_corrector</code><span class="sig-paren">(</span><em class="sig-param">design</em>, <em class="sig-param">calc_beta</em>, <em class="sig-param">order=1</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nipy/algorithms/statistics/models/regression.html#ar_bias_corrector"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.ar_bias_corrector" title="Permalink to this definition">¶</a></dt>
<dd><p>Return bias correcting matrix for <cite>design</cite> and AR order <cite>order</cite></p>
<p>There is a slight bias in the rho estimates on residuals due to the
correlations induced in the residuals by fitting a linear model.  See
<a class="reference internal" href="#worsley20026" id="id8"><span>[Worsley20026]</span></a>.</p>
<p>This routine implements the bias correction described in appendix A.1 of
<a class="reference internal" href="#worsley20026" id="id9"><span>[Worsley20026]</span></a>.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>design</strong> : array</p>
<blockquote>
<div><p>Design matrix</p>
</div></blockquote>
<p><strong>calc_beta</strong> : array</p>
<blockquote>
<div><p>Moore-Penrose pseudoinverse of the (maybe) whitened design matrix.
This is the matrix that, when applied to the (maybe whitened) data,
produces the betas.</p>
</div></blockquote>
<p><strong>order</strong> : int, optional</p>
<blockquote>
<div><p>Order p of AR(p) process</p>
</div></blockquote>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>invM</strong> : array</p>
<blockquote>
<div><p>Matrix to bias correct estimated covariance matrix
in calculating the AR coefficients</p>
</div></blockquote>
</dd>
</dl>
<p class="rubric">References</p>
<dl class="citation">
<dt class="label" id="worsley20026"><span class="brackets">Worsley20026</span><span class="fn-backref">(<a href="#id8">1</a>,<a href="#id9">2</a>,<a href="#id10">3</a>)</span></dt>
<dd><p>K.J. Worsley, C.H. Liao, J. Aston, V. Petre, G.H. Duncan,
F. Morales, A.C. Evans (2002) A General Statistical Analysis for fMRI
Data.  Neuroimage 15:1:15</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="nipy.algorithms.statistics.models.regression.isestimable">
<code class="sig-prename descclassname">nipy.algorithms.statistics.models.regression.</code><code class="sig-name descname">isestimable</code><span class="sig-paren">(</span><em class="sig-param">C</em>, <em class="sig-param">D</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nipy/algorithms/statistics/models/regression.html#isestimable"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.isestimable" title="Permalink to this definition">¶</a></dt>
<dd><p>True if (Q, P) contrast <cite>C</cite> is estimable for (N, P) design <cite>D</cite></p>
<p>From an Q x P contrast matrix <cite>C</cite> and an N x P design matrix <cite>D</cite>, checks if
the contrast <cite>C</cite> is estimable by looking at the rank of <code class="docutils literal notranslate"><span class="pre">vstack([C,D])</span></code>
and verifying it is the same as the rank of <cite>D</cite>.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>C</strong> : (Q, P) array-like</p>
<blockquote>
<div><p>contrast matrix. If <cite>C</cite> has is 1 dimensional assume shape (1, P)</p>
</div></blockquote>
<p><strong>D: (N, P) array-like</strong></p>
<blockquote>
<div><p>design matrix</p>
</div></blockquote>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>tf</strong> : bool</p>
<blockquote>
<div><p>True if the contrast <cite>C</cite> is estimable on design <cite>D</cite></p>
</div></blockquote>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">D</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
<span class="gp">... </span>              <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
<span class="gp">... </span>              <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span><span class="o">.</span><span class="n">T</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">isestimable</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">D</span><span class="p">)</span>
<span class="go">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">isestimable</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">D</span><span class="p">)</span>
<span class="go">True</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="nipy.algorithms.statistics.models.regression.yule_walker">
<code class="sig-prename descclassname">nipy.algorithms.statistics.models.regression.</code><code class="sig-name descname">yule_walker</code><span class="sig-paren">(</span><em class="sig-param">X</em>, <em class="sig-param">order=1</em>, <em class="sig-param">method='unbiased'</em>, <em class="sig-param">df=None</em>, <em class="sig-param">inv=False</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nipy/algorithms/statistics/models/regression.html#yule_walker"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nipy.algorithms.statistics.models.regression.yule_walker" title="Permalink to this definition">¶</a></dt>
<dd><p>Estimate AR(p) parameters from a sequence X using Yule-Walker equation.</p>
<p>unbiased or maximum-likelihood estimator (mle)</p>
<p>See, for example:</p>
<p><a class="reference external" href="http://en.wikipedia.org/wiki/Autoregressive_moving_average_model">http://en.wikipedia.org/wiki/Autoregressive_moving_average_model</a></p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> :  ndarray of shape(n)</p>
<p><strong>order</strong> : int, optional</p>
<blockquote>
<div><p>Order of AR process.</p>
</div></blockquote>
<p><strong>method</strong> : str, optional</p>
<blockquote>
<div><p>Method can be “unbiased” or “mle” and this determines denominator in
estimate of autocorrelation function (ACF) at lag k. If “mle”, the
denominator is n=X.shape[0], if “unbiased” the denominator is n-k.</p>
</div></blockquote>
<p><strong>df</strong> : int, optional</p>
<blockquote>
<div><p>Specifies the degrees of freedom. If df is supplied, then it is assumed
the X has df degrees of freedom rather than n.</p>
</div></blockquote>
<p><strong>inv</strong> : bool, optional</p>
<blockquote>
<div><p>Whether to return the inverse of the R matrix (see code)</p>
</div></blockquote>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>rho</strong> : (<cite>order</cite>,) ndarray</p>
<p><strong>sigma</strong> : int</p>
<blockquote>
<div><p>standard deviation of the residuals after fit</p>
</div></blockquote>
<p><strong>R_inv</strong> : ndarray</p>
<blockquote>
<div><p>If <cite>inv</cite> is True, also return the inverse of the R matrix</p>
</div></blockquote>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>See also
<a class="reference external" href="http://en.wikipedia.org/wiki/AR_model#Calculation_of_the_AR_parameters">http://en.wikipedia.org/wiki/AR_model#Calculation_of_the_AR_parameters</a></p>
</dd></dl>

</div>
</div>


           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright J. Taylor and others
      <span class="lastupdated">
        Last updated on Sep 24, 2019.
      </span>

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>