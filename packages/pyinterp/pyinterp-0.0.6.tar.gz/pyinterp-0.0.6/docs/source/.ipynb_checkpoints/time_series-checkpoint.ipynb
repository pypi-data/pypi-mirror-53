{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolation of a time series\n",
    "\n",
    "This example shows how to interpolate a time series using the library.\n",
    "\n",
    "In this example, we consider the time series of MSLA maps distributed by AVISO/CMEMS.\n",
    "\n",
    "## Initialize Dataset\n",
    "\n",
    "Here we load the dataset from the zarr store. Note that this very large dataset initializes nearly instantly, and we can see the full list of variables and coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:    (latitude: 720, longitude: 1440, nv: 2, time: 8901)\n",
       "Coordinates:\n",
       "    crs        int32 ...\n",
       "    lat_bnds   (time, latitude, nv) float32 dask.array<chunksize=(5, 720, 2), meta=np.ndarray>\n",
       "  * latitude   (latitude) float32 -89.875 -89.625 -89.375 ... 89.625 89.875\n",
       "    lon_bnds   (longitude, nv) float32 dask.array<chunksize=(1440, 2), meta=np.ndarray>\n",
       "  * longitude  (longitude) float32 0.125 0.375 0.625 ... 359.375 359.625 359.875\n",
       "  * nv         (nv) int32 0 1\n",
       "  * time       (time) datetime64[ns] 1993-01-01 1993-01-02 ... 2017-05-15\n",
       "Data variables:\n",
       "    adt        (time, latitude, longitude) float64 dask.array<chunksize=(5, 720, 1440), meta=np.ndarray>\n",
       "    err        (time, latitude, longitude) float64 dask.array<chunksize=(5, 720, 1440), meta=np.ndarray>\n",
       "    sla        (time, latitude, longitude) float64 dask.array<chunksize=(5, 720, 1440), meta=np.ndarray>\n",
       "    ugos       (time, latitude, longitude) float64 dask.array<chunksize=(5, 720, 1440), meta=np.ndarray>\n",
       "    ugosa      (time, latitude, longitude) float64 dask.array<chunksize=(5, 720, 1440), meta=np.ndarray>\n",
       "    vgos       (time, latitude, longitude) float64 dask.array<chunksize=(5, 720, 1440), meta=np.ndarray>\n",
       "    vgosa      (time, latitude, longitude) float64 dask.array<chunksize=(5, 720, 1440), meta=np.ndarray>\n",
       "Attributes:\n",
       "    Conventions:                     CF-1.6\n",
       "    Metadata_Conventions:            Unidata Dataset Discovery v1.0\n",
       "    cdm_data_type:                   Grid\n",
       "    comment:                         Sea Surface Height measured by Altimetry...\n",
       "    contact:                         servicedesk.cmems@mercator-ocean.eu\n",
       "    creator_email:                   servicedesk.cmems@mercator-ocean.eu\n",
       "    creator_name:                    CMEMS - Sea Level Thematic Assembly Center\n",
       "    creator_url:                     http://marine.copernicus.eu\n",
       "    date_created:                    2014-02-26T16:09:13Z\n",
       "    date_issued:                     2014-01-06T00:00:00Z\n",
       "    date_modified:                   2015-11-10T19:42:51Z\n",
       "    geospatial_lat_max:              89.875\n",
       "    geospatial_lat_min:              -89.875\n",
       "    geospatial_lat_resolution:       0.25\n",
       "    geospatial_lat_units:            degrees_north\n",
       "    geospatial_lon_max:              359.875\n",
       "    geospatial_lon_min:              0.125\n",
       "    geospatial_lon_resolution:       0.25\n",
       "    geospatial_lon_units:            degrees_east\n",
       "    geospatial_vertical_max:         0.0\n",
       "    geospatial_vertical_min:         0.0\n",
       "    geospatial_vertical_positive:    down\n",
       "    geospatial_vertical_resolution:  point\n",
       "    geospatial_vertical_units:       m\n",
       "    history:                         2014-02-26T16:09:13Z: created by DUACS D...\n",
       "    institution:                     CLS, CNES\n",
       "    keywords:                        Oceans > Ocean Topography > Sea Surface ...\n",
       "    keywords_vocabulary:             NetCDF COARDS Climate and Forecast Stand...\n",
       "    license:                         http://marine.copernicus.eu/web/27-servi...\n",
       "    platform:                        ERS-1, Topex/Poseidon\n",
       "    processing_level:                L4\n",
       "    product_version:                 5.0\n",
       "    project:                         COPERNICUS MARINE ENVIRONMENT MONITORING...\n",
       "    references:                      http://marine.copernicus.eu\n",
       "    source:                          Altimetry measurements\n",
       "    ssalto_duacs_comment:            The reference mission used for the altim...\n",
       "    standard_name_vocabulary:        NetCDF Climate and Forecast (CF) Metadat...\n",
       "    summary:                         SSALTO/DUACS Delayed-Time Level-4 sea su...\n",
       "    time_coverage_duration:          P1D\n",
       "    time_coverage_end:               1993-01-01T12:00:00Z\n",
       "    time_coverage_resolution:        P1D\n",
       "    time_coverage_start:             1992-12-31T12:00:00Z\n",
       "    title:                           DT merged all satellites Global Ocean Gr..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import intake\n",
    "cat = intake.Catalog(\"https://raw.githubusercontent.com/pangeo-data/pangeo-datastore/master/intake-catalogs/ocean.yaml\")\n",
    "ds = cat[\"sea_surface_height\"].to_dask()\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The \"crs\" coordinates must be removed to avoid a bug when determining the 3rd axis.\n",
    "ds = ds.drop(\"crs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle the time series\n",
    "\n",
    "We implement a class to handle a time series and on demand loading the data required to interpolate data over a specific time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyinterp.backends.xarray\n",
    "\n",
    "\n",
    "class GridSeries:\n",
    "    \"\"\"Handling of MSLA AVISO maps\"\"\"\n",
    "\n",
    "    def __init__(self, ds):\n",
    "        self.ds = ds\n",
    "        self.series, self.dt = self._load_ts()\n",
    "        \n",
    "    @staticmethod\n",
    "    def _is_sorted(array):\n",
    "        indices = np.argsort(array)\n",
    "        return np.all(indices == np.arange(len(indices)))\n",
    "\n",
    "    def _load_ts(self):\n",
    "        \"\"\"Loading the time series into memory.\"\"\"\n",
    "        time = self.ds.time\n",
    "        assert self._is_sorted(time)\n",
    "\n",
    "        series = pd.Series(time)\n",
    "        frequency = set(np.diff(series.values.astype(\"datetime64[s]\")).astype(\"int64\"))\n",
    "        if len(frequency) != 1:\n",
    "            raise RuntimeError(\n",
    "                \"Time series does not have a constant step between two \"\n",
    "                f\"grids: {frequency} seconds\")\n",
    "        return series, datetime.timedelta(seconds=float(frequency.pop()))\n",
    "    \n",
    "    def load_dataset(self, varname, start, end):\n",
    "        \"\"\"Loading the time series into memory for the defined period.\n",
    "\n",
    "        Args:\n",
    "            varname (str): Name of the variable to be loaded into memory.\n",
    "            start (datetime.datetime): Date of the first map to be loaded.\n",
    "            end (datetime.datetime): Date of the last map to be loaded.\n",
    "\n",
    "        Return:\n",
    "            pyinterp.backends.xarray.Grid3D: The interpolator handling the\n",
    "            interpolation of the grid series.\n",
    "        \"\"\"\n",
    "        if start < self.series.min() or end > self.series.max():\n",
    "            raise IndexError(\n",
    "                f\"period [{start}, {end}] out of range [{self.series.min()}, \"\n",
    "                f\"{self.series.max()}]\")\n",
    "        first = start - self.dt\n",
    "        last = end + self.dt\n",
    "\n",
    "        selected = self.series[(self.series >= first) & (self.series < last)]\n",
    "        print(f\"fetch data from {selected.min()} to {selected.max()}\")\n",
    "        \n",
    "        data_array = ds[varname].isel(time=selected.index)\n",
    "        return pyinterp.backends.xarray.Grid3D(data_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=ds[\"sla\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('time', 'latitude', 'longitude')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "\n",
    "Finally, the functions necessary to load the test set into memory are added. This file contains several columns defining the float identifier, the date of the measurement, the longitude and the latitude of the measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnes_jd_to_datetime(seconds):\n",
    "    \"\"\"Convert a date expressed in seconds since 1950 into a calendar\n",
    "    date.\"\"\"\n",
    "    return datetime.datetime.utcfromtimestamp(\n",
    "        ((seconds / 86400.0) - 7305.0) * 86400.0)\n",
    "\n",
    "\n",
    "def load_positions():\n",
    "    \"\"\"Loading and formatting the dataset.\"\"\"\n",
    "    df = pd.read_csv(\"../tests/dataset/positions.csv\",\n",
    "                     header=None,\n",
    "                     sep=r\";\",\n",
    "                     usecols=[0, 1, 2, 3],\n",
    "                     names=[\"id\", \"time\", \"lon\", \"lat\"],\n",
    "                     dtype=dict(id=np.uint32,\n",
    "                                time=np.float64,\n",
    "                                lon=np.float64,\n",
    "                                lat=np.float64))\n",
    "    df.mask(df == 1.8446744073709552e+19, np.nan, inplace=True)\n",
    "    df[\"time\"] = df[\"time\"].apply(cnes_jd_to_datetime)\n",
    "    df.set_index('time', inplace=True)\n",
    "    df[\"sla\"] = np.nan\n",
    "    return df.sort_index()\n",
    "\n",
    "df = load_positions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of interpolation\n",
    "\n",
    "We create the object that will handle the download of data for the periods required for the interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSeries(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below, allows to cluster the processing period into sub-periods in order to load the grids in blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def periods(df, grid_series, frequency='W'):\n",
    "    \"\"\"Return the list of periods covering the time series loaded in\n",
    "    memory.\"\"\"\n",
    "    period_start = df.groupby(\n",
    "        df.index.to_period(frequency))[\"sla\"].count().index\n",
    "\n",
    "    for start, end in zip(period_start, period_start[1:]):\n",
    "        start = start.to_timestamp()\n",
    "        if start < grid_series.series[0]:\n",
    "            start = grid_series.series[0]\n",
    "        end = end.to_timestamp()\n",
    "        yield start, end\n",
    "    yield end, df.index[-1] + grid_series.dt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the interpolation function is written for one of the sub-periods selected by the function `periods`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate(df, grid_series, start, end):\n",
    "    \"\"\"Interpolate the time series over the defined period.\"\"\"\n",
    "    interpolator = grid_series.load_dataset(\"sla\", start, end)\n",
    "    mask = (df.index >= start) & (df.index < end)\n",
    "    selected = df.loc[mask, [\"lon\", \"lat\"]]\n",
    "    df.loc[mask, [\"sla\"]] = interpolator.trivariate(dict(\n",
    "        longitude=selected[\"lon\"].values,\n",
    "        latitude=selected[\"lat\"].values,\n",
    "        time=selected.index.values),\n",
    "        interpolator=\"inverse_distance_weighting\",\n",
    "        num_threads=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetch data from 2015-12-27 00:00:00 to 2016-01-04 00:00:00\n",
      "fetch data from 2016-01-03 00:00:00 to 2016-01-11 00:00:00\n",
      "fetch data from 2016-01-10 00:00:00 to 2016-01-18 00:00:00\n",
      "fetch data from 2016-01-17 00:00:00 to 2016-01-25 00:00:00\n",
      "fetch data from 2016-01-24 00:00:00 to 2016-02-01 00:00:00\n",
      "fetch data from 2016-01-31 00:00:00 to 2016-02-08 00:00:00\n",
      "fetch data from 2016-02-07 00:00:00 to 2016-02-15 00:00:00\n",
      "fetch data from 2016-02-14 00:00:00 to 2016-02-22 00:00:00\n",
      "fetch data from 2016-02-21 00:00:00 to 2016-02-29 00:00:00\n",
      "fetch data from 2016-02-28 00:00:00 to 2016-03-07 00:00:00\n",
      "fetch data from 2016-03-06 00:00:00 to 2016-03-14 00:00:00\n",
      "fetch data from 2016-03-13 00:00:00 to 2016-03-21 00:00:00\n",
      "fetch data from 2016-03-20 00:00:00 to 2016-03-28 00:00:00\n",
      "fetch data from 2016-03-27 00:00:00 to 2016-04-04 00:00:00\n",
      "fetch data from 2016-04-03 00:00:00 to 2016-04-11 00:00:00\n",
      "fetch data from 2016-04-10 00:00:00 to 2016-04-18 00:00:00\n",
      "fetch data from 2016-04-17 00:00:00 to 2016-04-25 00:00:00\n",
      "fetch data from 2016-04-24 00:00:00 to 2016-05-02 00:00:00\n",
      "fetch data from 2016-05-01 00:00:00 to 2016-05-09 00:00:00\n",
      "fetch data from 2016-05-08 00:00:00 to 2016-05-16 00:00:00\n",
      "fetch data from 2016-05-15 00:00:00 to 2016-05-23 00:00:00\n",
      "fetch data from 2016-05-22 00:00:00 to 2016-05-30 00:00:00\n",
      "fetch data from 2016-05-29 00:00:00 to 2016-06-06 00:00:00\n",
      "fetch data from 2016-06-05 00:00:00 to 2016-06-13 00:00:00\n",
      "fetch data from 2016-06-12 00:00:00 to 2016-06-20 00:00:00\n",
      "fetch data from 2016-06-19 00:00:00 to 2016-06-27 00:00:00\n",
      "fetch data from 2016-06-26 00:00:00 to 2016-07-04 00:00:00\n",
      "fetch data from 2016-07-03 00:00:00 to 2016-07-11 00:00:00\n",
      "fetch data from 2016-07-10 00:00:00 to 2016-07-18 00:00:00\n",
      "fetch data from 2016-07-17 00:00:00 to 2016-07-25 00:00:00\n",
      "fetch data from 2016-07-24 00:00:00 to 2016-08-01 00:00:00\n",
      "fetch data from 2016-07-31 00:00:00 to 2016-08-08 00:00:00\n"
     ]
    }
   ],
   "source": [
    "for start, end in periods(df, gs):\n",
    "    interpolate(df, gs, start, end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of the SLA for a float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_id = 62423050\n",
    "selected_float = df[df.id == float_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "%matplotlib inline\n",
    "\n",
    "rule = mdates.rrulewrapper(mdates.MONTHLY, bymonthday=1, interval=1)\n",
    "formatter = mdates.DateFormatter('%b/%m/%d')\n",
    "loc = mdates.RRuleLocator(rule)\n",
    "\n",
    "def plot(ax, x, y, title):\n",
    "    ax.plot(x, y)\n",
    "    ax.xaxis.set_major_locator(loc)\n",
    "    ax.xaxis.set_major_formatter(formatter)\n",
    "    ax.grid(True)\n",
    "    ax.set_title(title)\n",
    "    labels = ax.get_xticklabels()\n",
    "    plt.setp(labels, rotation=30, fontsize=10)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9, 5))\n",
    "ax = fig.add_subplot(131)\n",
    "plot(ax, selected_float.index, selected_float.sla, \"SLA\")\n",
    "ax = fig.add_subplot(132)\n",
    "plot(ax, selected_float.index, selected_float.lon, \"longitude\")\n",
    "ax = fig.add_subplot(133)\n",
    "plot(ax, selected_float.index, selected_float.lat, \"latitude\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
